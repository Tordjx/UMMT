{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "efb10bae-68df-4d91-9e10-2cecaf169e71",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\valen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# !git clone https://github.com/tordjx/ummt.git\n",
    "# import os \n",
    "# os.chdir(\"ummt/Core model files\")\n",
    "# os.getcwd()\n",
    "!pip install matplotlib --quiet\n",
    "!pip install livelossplot --quiet\n",
    "!pip install nltk --quiet\n",
    "!pip install torchtext --quiet\n",
    "#mc cp s3/tordjx ummt/Core*model*files --recursive\n",
    "# watch -n 0.5 nvidia-smi\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "image_bool = False\n",
    "load_model = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f7ee3bb-7c9c-47e3-ae7f-064be78e7002",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#%%\n",
    "from Modele_decodeur_maison import *\n",
    "from Pipeline import *\n",
    "from Trainer import * \n",
    "from greedy_beam_search import *\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "batch_size = 256\n",
    "\n",
    "# Texts\n",
    "tokenized_fr,tokenized_en, vocab_fr,vocab_en = get_train_data_nouveau(batch_size,True)\n",
    "tokenized_val_fr,tokenized_val_en, _,_ = get_train_data_nouveau(batch_size,False)\n",
    "#Data non batchés\n",
    "n_token_fr = len(vocab_fr.keys())\n",
    "n_token_en = len(vocab_en.keys())\n",
    "\n",
    "inv_map_en = {v: k for k, v in vocab_en.items()}\n",
    "inv_map_fr = {v: k for k, v in vocab_fr.items()}\n",
    "\n",
    "n_head =8\n",
    "num_encoder_layers = 4\n",
    "num_decoder_layers = 4\n",
    "dim_feedforward = 512\n",
    "dropout = 0.1\n",
    "activation = nn.Softmax(dim=2)\n",
    "embedding_dim = 128\n",
    "\n",
    "model_fr = Modèle(n_token_fr,embedding_dim,n_head, num_encoder_layers,num_decoder_layers,dim_feedforward,dropout,activation,vocab_fr[\"TOKEN_VIDE\"],vocab_fr[\"DEBUT_DE_PHRASE\"],vocab_fr[\"FIN_DE_PHRASE\"],False,'').to(device)\n",
    "model_en = Modèle(n_token_en,embedding_dim,n_head, num_encoder_layers,num_decoder_layers,dim_feedforward,dropout,activation,vocab_en[\"TOKEN_VIDE\"],vocab_en[\"DEBUT_DE_PHRASE\"],vocab_en[\"FIN_DE_PHRASE\"],False,'').to(device)\n",
    "model_fr.feedforward= model_en.feedforward\n",
    "for i in range(3):\n",
    "  model_fr.decoder.layers[i]= model_en.decoder.layers[i]\n",
    "  model_fr.encoder.layers[i]= model_en.encoder.layers[i]\n",
    "prefix = \"fulltry\"\n",
    "if load_model : \n",
    "    model_en.load_state_dict(torch.load(\"tordjx/\"+prefix+\"_en\"))\n",
    "    model_fr.load_state_dict(torch.load(\"tordjx/\"+prefix+\"_fr\"))\n",
    "else : \n",
    "    with open(\"logs.txt\",'w') as logs :\n",
    "        logs.write(\"\")\n",
    "        logs.close()\n",
    "if image_bool :\n",
    "    train_features  = np.load(\"tordjx/train-resnet50-res4frelu.npy\")\n",
    "    val_features = np.load(\"tordjx/val-resnet50-res4frelu.npy\")\n",
    "    train_features = torch.from_numpy(train_features)\n",
    "    val_features = torch.from_numpy(val_features)\n",
    "    train_data_fr = [tokenized_fr, train_features]\n",
    "    train_data_en = [tokenized_en, train_features]\n",
    "    val_data_fr = [tokenized_val_fr, val_features]\n",
    "    val_data_en = [tokenized_val_en, val_features]\n",
    "def save_model(model_en,model_fr,prefix):\n",
    "    torch.save(model_fr.state_dict(), \"tordjx/\"+prefix+\"_fr\")\n",
    "    torch.save(model_en.state_dict(), \"tordjx/\"+prefix+\"_en\")\n",
    "    import os\n",
    "    import s3fs\n",
    "    !pip install pandas\n",
    "    import pandas\n",
    "    # Create filesystem object\n",
    "    S3_ENDPOINT_URL = \"https://\" + os.environ[\"AWS_S3_ENDPOINT\"]\n",
    "    fs = s3fs.S3FileSystem(client_kwargs={'endpoint_url': S3_ENDPOINT_URL})\n",
    "    fs.upload(\"tordjx/\"+prefix+\"_fr\",\"tordjx/\"+prefix+\"_fr\")\n",
    "    fs.upload(\"tordjx/\"+prefix+\"_en\",\"tordjx/\"+prefix+\"_en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d6a5f97-980c-4051-a19b-422479724a88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Programmes\\Python\\lib\\site-packages\\torch\\nn\\functional.py:4999: UserWarning: Support for mismatched src_key_padding_mask and src_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n",
      "d:\\Programmes\\Python\\lib\\site-packages\\torch\\nn\\functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'batch_size' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 18\u001b[0m\n\u001b[0;32m     16\u001b[0m     mixed_train(val_data_en,val_data_fr,inv_map_en,inv_map_fr,model_fr,model_en,train_data_fr,train_data_en,\u001b[39m60\u001b[39m,batch_size, \u001b[39mTrue\u001b[39;00m,\u001b[39m1\u001b[39m\u001b[39m/\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[0;32m     17\u001b[0m \u001b[39melse\u001b[39;00m :\n\u001b[1;32m---> 18\u001b[0m     mixed_train(tokenized_val_en,tokenized_val_fr,inv_map_en,inv_map_fr,model_fr,model_en,tokenized_fr,tokenized_en,\u001b[39m60\u001b[39;49m,batch_size, \u001b[39mFalse\u001b[39;49;00m,\u001b[39m1\u001b[39;49m\u001b[39m/\u001b[39;49m\u001b[39m2\u001b[39;49m)\n\u001b[0;32m     19\u001b[0m torch\u001b[39m.\u001b[39msave(model_fr\u001b[39m.\u001b[39mstate_dict(), \u001b[39m\"\u001b[39m\u001b[39mtordjx/\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m+\u001b[39mprefix\u001b[39m+\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m_fr\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     20\u001b[0m torch\u001b[39m.\u001b[39msave(model_en\u001b[39m.\u001b[39mstate_dict(), \u001b[39m\"\u001b[39m\u001b[39mtordjx/\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m+\u001b[39mprefix\u001b[39m+\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m_en\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32md:\\Valentin\\Documents\\ENSAE\\STATAPP\\UMMT\\Core model files\\Trainer.py:99\u001b[0m, in \u001b[0;36mmixed_train\u001b[1;34m(val_data_en, val_data_fr, inv_map_en, inv_map_fr, model_fr, model_en, train_data_fr, train_data_en, n_iter, batch_size, image_bool, part_auto_encoding)\u001b[0m\n\u001b[0;32m     97\u001b[0m tokenized_val_fr \u001b[39m=\u001b[39m val_data_fr[\u001b[39m0\u001b[39m]\n\u001b[0;32m     98\u001b[0m \u001b[39mfor\u001b[39;00m i_iter \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_iter):\n\u001b[1;32m---> 99\u001b[0m     save_dataframe_eval(model_fr,model_en,tokenized_val_en,tokenized_val_fr,inv_map_en,inv_map_fr,image_bool,i_iter)\n\u001b[0;32m    100\u001b[0m     model_en\u001b[39m.\u001b[39mcurr_epoch \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m\n\u001b[0;32m    101\u001b[0m     model_fr\u001b[39m.\u001b[39mcurr_epoch \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m\n",
      "File \u001b[1;32md:\\Valentin\\Documents\\ENSAE\\STATAPP\\UMMT\\Core model files\\evaluateur.py:197\u001b[0m, in \u001b[0;36msave_dataframe_eval\u001b[1;34m(model_fr, model_en, tokenized_val_en, tokenized_val_fr, inv_map_en, inv_map_fr, image_bool, epoch)\u001b[0m\n\u001b[0;32m    196\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msave_dataframe_eval\u001b[39m(model_fr,model_en,tokenized_val_en,tokenized_val_fr,inv_map_en,inv_map_fr,image_bool,epoch \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m):\n\u001b[1;32m--> 197\u001b[0m     df \u001b[39m=\u001b[39m dataframe_eval(model_fr,model_en,tokenized_val_en,tokenized_val_fr,inv_map_en,inv_map_fr,image_bool)\n\u001b[0;32m    198\u001b[0m     df[\u001b[39m\"\u001b[39m\u001b[39mbleu_en_fr\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m row: bleu(row,\u001b[39m\"\u001b[39m\u001b[39men\u001b[39m\u001b[39m\"\u001b[39m), axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m    199\u001b[0m     df[\u001b[39m\"\u001b[39m\u001b[39mbleu_fr_en\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m row: bleu(row,\u001b[39m\"\u001b[39m\u001b[39mfr\u001b[39m\u001b[39m\"\u001b[39m), axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32md:\\Valentin\\Documents\\ENSAE\\STATAPP\\UMMT\\Core model files\\evaluateur.py:127\u001b[0m, in \u001b[0;36mdataframe_eval\u001b[1;34m(model_fr, model_en, val_data_en, val_data_fr, inv_map_en, inv_map_fr, image_bool)\u001b[0m\n\u001b[0;32m    125\u001b[0m \u001b[39melse\u001b[39;00m :\n\u001b[0;32m    126\u001b[0m     val_data_en,val_data_fr \u001b[39m=\u001b[39m val_data_en,val_data_fr\n\u001b[1;32m--> 127\u001b[0m batched_data_en,batched_data_fr\u001b[39m=\u001b[39mbatchify([val_data_en,val_data_fr],batch_size,image_bool,conservative\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,permute \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m    128\u001b[0m \u001b[39mif\u001b[39;00m image_bool:\n\u001b[0;32m    129\u001b[0m     src,features \u001b[39m=\u001b[39m batched_data_en\n",
      "\u001b[1;31mNameError\u001b[0m: name 'batch_size' is not defined"
     ]
    }
   ],
   "source": [
    "#On va entrainer 4 modeles : avec/sans images, avec/sans teacher forcing\n",
    "teacher_forcing = False\n",
    "bools = [True,False]\n",
    "for image_bool in bools : \n",
    "    prefix = str(image_bool)+str(teacher_forcing)\n",
    "    model_fr = Modèle(n_token_fr,embedding_dim,n_head, num_encoder_layers,num_decoder_layers,dim_feedforward,dropout,activation,vocab_fr[\"TOKEN_VIDE\"],vocab_fr[\"DEBUT_DE_PHRASE\"],vocab_fr[\"FIN_DE_PHRASE\"],teacher_forcing,prefix).to(device)\n",
    "    model_en = Modèle(n_token_en,embedding_dim,n_head, num_encoder_layers,num_decoder_layers,dim_feedforward,dropout,activation,vocab_en[\"TOKEN_VIDE\"],vocab_en[\"DEBUT_DE_PHRASE\"],vocab_en[\"FIN_DE_PHRASE\"],teacher_forcing,prefix).to(device)\n",
    "    model_fr.feedforward= model_en.feedforward\n",
    "    for i in range(3):\n",
    "        model_fr.decoder.layers[i]= model_en.decoder.layers[i]\n",
    "        model_fr.encoder.layers[i]= model_en.encoder.layers[i]\n",
    "    with open(prefix+\"logs.txt\",'w') as logs :\n",
    "        logs.write(\"\")\n",
    "        logs.close()\n",
    "    if image_bool :\n",
    "        mixed_train(val_data_en,val_data_fr,inv_map_en,inv_map_fr,model_fr,model_en,train_data_fr,train_data_en,60,batch_size, True,1/2)\n",
    "    else :\n",
    "        mixed_train(tokenized_val_en,tokenized_val_fr,inv_map_en,inv_map_fr,model_fr,model_en,tokenized_fr,tokenized_en,60,batch_size, False,1/2)\n",
    "    torch.save(model_fr.state_dict(), \"tordjx/\"+prefix+\"_fr\")\n",
    "    torch.save(model_en.state_dict(), \"tordjx/\"+prefix+\"_en\")\n",
    "    pd.DataFrame(model_en.loss_list).to_csv(\"tordjx/\"+prefix+\"loss_en.csv\")\n",
    "    pd.DataFrame(model_fr.loss_list).to_csv(\"tordjx/\"+prefix+\"loss_fr.csv\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba8bf1c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Programmes\\Python\\lib\\site-packages\\torch\\nn\\functional.py:4999: UserWarning: Support for mismatched src_key_padding_mask and src_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n",
      "d:\\Programmes\\Python\\lib\\site-packages\\torch\\nn\\functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.7672,  0.6324, -2.2318,  ...,  3.2074, -1.5971,  0.5021],\n",
       "         [-1.8711, -1.1175, -1.7387,  ...,  1.9533, -0.6104,  0.6691],\n",
       "         [-2.5070,  0.7686, -1.9276,  ...,  2.5660, -0.9294,  0.5455],\n",
       "         ...,\n",
       "         [-2.2627,  0.6238, -1.7786,  ...,  2.0241, -1.1009,  1.6198],\n",
       "         [-1.3301,  0.1612, -1.9499,  ...,  1.6829, -1.0265,  1.4370],\n",
       "         [-1.4545,  0.3633, -1.3041,  ...,  1.8026, -1.3168,  1.2916]],\n",
       "\n",
       "        [[ 0.4346, -1.8247, -2.3207,  ...,  2.7586, -0.3141, -0.0385],\n",
       "         [-0.2487, -0.9901, -1.9830,  ...,  2.4211, -0.4552,  0.7975],\n",
       "         [-0.5314, -0.5862, -1.6345,  ...,  2.3901, -0.4316,  0.7297],\n",
       "         ...,\n",
       "         [-1.5806, -0.3029, -1.5977,  ...,  3.0642, -1.3033,  0.8397],\n",
       "         [-1.0783, -0.6023, -2.4334,  ...,  1.8654, -0.2117,  0.9085],\n",
       "         [-1.1355, -0.4256, -1.5829,  ...,  3.0951, -1.1788,  1.3038]],\n",
       "\n",
       "        [[-0.4734, -1.1233, -2.7201,  ...,  3.4664,  0.1761,  0.8877],\n",
       "         [-1.1830, -1.4720, -1.6237,  ...,  2.1842, -0.6953, -0.3005],\n",
       "         [-1.6022, -1.3293, -1.7169,  ...,  1.5716, -0.1938,  0.9530],\n",
       "         ...,\n",
       "         [-2.2047, -1.1574, -1.9742,  ...,  2.6724,  0.5779,  1.2782],\n",
       "         [-1.8340, -0.8617, -1.7533,  ...,  2.0062,  0.3763,  0.3884],\n",
       "         [-1.8117, -0.9105, -1.2268,  ...,  1.7089, -0.2627,  0.6535]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-1.7445, -1.0802, -3.8266,  ...,  2.8141, -0.1897,  0.1665],\n",
       "         [-1.4566, -1.8985, -3.2853,  ...,  2.9236, -0.1663, -0.1350],\n",
       "         [-0.9470, -1.6510, -3.5528,  ...,  2.8092, -0.2734,  0.0116],\n",
       "         ...,\n",
       "         [ 0.4313, -1.4283, -2.5295,  ...,  2.1533, -0.5493,  0.8083],\n",
       "         [ 0.5044, -0.7629, -2.9392,  ...,  1.6745,  0.2206,  0.4683],\n",
       "         [-0.3311, -1.0687, -3.0374,  ...,  1.7406,  0.0568,  0.4826]],\n",
       "\n",
       "        [[-0.7750, -1.4568, -3.1031,  ...,  2.6716, -0.4005,  0.4410],\n",
       "         [ 0.3063, -1.9225, -2.6522,  ...,  3.1562, -0.5556,  0.4125],\n",
       "         [-0.4148, -1.9725, -2.8228,  ...,  2.2459, -0.0563,  0.9737],\n",
       "         ...,\n",
       "         [-0.6264, -0.7993, -2.9419,  ...,  2.6457,  0.2646,  1.3802],\n",
       "         [-0.4947, -1.6511, -3.1860,  ...,  2.0097,  0.7460,  1.2256],\n",
       "         [-0.5192, -1.0773, -2.6038,  ...,  2.2966,  0.0607,  1.2168]],\n",
       "\n",
       "        [[ 0.0483, -1.3987, -3.2099,  ...,  1.8952, -0.8749, -0.2009],\n",
       "         [ 0.3864, -1.3319, -2.1818,  ...,  2.6513, -0.0176, -0.2622],\n",
       "         [-0.2619, -1.4960, -3.2397,  ...,  2.8554,  1.0315,  0.2707],\n",
       "         ...,\n",
       "         [-0.2402, -1.4280, -1.8668,  ...,  1.6698,  0.7460,  0.8663],\n",
       "         [-0.9940, -1.3050, -1.7044,  ...,  1.9627,  0.7954,  1.1541],\n",
       "         [-0.7783, -1.0880, -1.4609,  ...,  1.9946,  0.3037,  1.6723]]],\n",
       "       grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_en(tokenized_val_en[:batch_size],False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09bd9a6-d152-4dfa-984f-f9393a53199b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# if image_bool :\n",
    "#     loss_list = learning_rate_finder(val_data_en,val_data_fr,inv_map_en,inv_map_fr,model_fr,model_en,train_data_fr,train_data_en,150,batch_size, True,1/2)\n",
    "# else :\n",
    "#     loss_list = learning_rate_finder(tokenized_val_en,tokenized_val_fr,inv_map_en,inv_map_fr,model_fr,model_en,tokenized_fr,tokenized_en,150,batch_size, False,1/2)\n",
    "\n",
    "lrs = [10**(-10)*3**n for n in range(len(loss_list))]\n",
    "cut = 11\n",
    "begin = 10\n",
    "plt.plot(lrs[begin:-cut],loss_list[begin:-cut])\n",
    "plt.xscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033da8a6-1d98-4eea-be9a-ba67221f6780",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def moving_average(a, n=1,tail = 0) :\n",
    "    ret = np.cumsum(a, dtype=float)\n",
    "    ret[n:] = ret[n:] - ret[:-n]\n",
    "    \n",
    "    return (ret[n - 1:] / n)[tail:]\n",
    "\n",
    "plt.plot(moving_average(model_fr.loss_list))\n",
    "plt.plot(moving_average(model_en.loss_list))\n",
    "print(len(model_fr.loss_list)/(29000/batch_size))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a62a7311-d658-4917-8e00-35d585778982",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenized_val_en = val_data_en[0]\n",
    "i = np.random.randint(len(tokenized_val_en)//batch_size)\n",
    "j = np.random.randint(batch_size)\n",
    "src,features,tgt = donne_random(i,j,val_data_en,val_data_fr,batch_size,True)\n",
    "features = features.to(device,dtype=torch.float32)\n",
    "data = [src,features]\n",
    "# OUT= traduit('greedy',model_en,model_fr,data, inv_map_en,True,tgt[j],inv_map_fr,j)\n",
    "OUT = greedy_beam_search.CCF_greedy(model_en,model_fr,src, features, True)\n",
    "values,indices = torch.topk(OUT,2,2)\n",
    "print(values)\n",
    "print(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5ca3ea82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in d:\\programmes\\python\\lib\\site-packages (4.7.0.72)\n",
      "Requirement already satisfied: numpy>=1.21.2 in d:\\programmes\\python\\lib\\site-packages (from opencv-python) (1.24.2)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Cannot handle this data type: (1, 1, 3), <f8",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32md:\\Programmes\\Python\\lib\\site-packages\\PIL\\Image.py:3080\u001b[0m, in \u001b[0;36mfromarray\u001b[1;34m(obj, mode)\u001b[0m\n\u001b[0;32m   3079\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 3080\u001b[0m     mode, rawmode \u001b[39m=\u001b[39m _fromarray_typemap[typekey]\n\u001b[0;32m   3081\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "\u001b[1;31mKeyError\u001b[0m: ((1, 1, 3), '<f8')",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[57], line 93\u001b[0m\n\u001b[0;32m     91\u001b[0m     \u001b[39mreturn\u001b[39;00m image\n\u001b[0;32m     92\u001b[0m \u001b[39mfor\u001b[39;00m images \u001b[39min\u001b[39;00m get_all_images_from_folder(\u001b[39m\"\u001b[39m\u001b[39mD:/Valentin/Desktop/Images ummt/raw/\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m---> 93\u001b[0m     save_image(blank_image(open_image(\u001b[39m\"\u001b[39;49m\u001b[39mD:/Valentin/Desktop/Images ummt/raw/\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m+\u001b[39;49mimages)),\u001b[39m\"\u001b[39m\u001b[39mD:/Valentin/Desktop/Images ummt/blank/\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m+\u001b[39mimages)\n",
      "Cell \u001b[1;32mIn[57], line 89\u001b[0m, in \u001b[0;36mblank_image\u001b[1;34m(image)\u001b[0m\n\u001b[0;32m     87\u001b[0m image \u001b[39m=\u001b[39m (image\u001b[39m*\u001b[39m\u001b[39m255\u001b[39m)\u001b[39m.\u001b[39mastype(np\u001b[39m.\u001b[39muint8)\n\u001b[0;32m     88\u001b[0m image \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros(image\u001b[39m.\u001b[39mshape)\n\u001b[1;32m---> 89\u001b[0m image \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39;49mfromarray(image)\n\u001b[0;32m     90\u001b[0m image \u001b[39m=\u001b[39m transforms\u001b[39m.\u001b[39mToTensor()(image)\n\u001b[0;32m     91\u001b[0m \u001b[39mreturn\u001b[39;00m image\n",
      "File \u001b[1;32md:\\Programmes\\Python\\lib\\site-packages\\PIL\\Image.py:3083\u001b[0m, in \u001b[0;36mfromarray\u001b[1;34m(obj, mode)\u001b[0m\n\u001b[0;32m   3081\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m   3082\u001b[0m         msg \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mCannot handle this data type: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m typekey\n\u001b[1;32m-> 3083\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(msg) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[0;32m   3084\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   3085\u001b[0m     rawmode \u001b[39m=\u001b[39m mode\n",
      "\u001b[1;31mTypeError\u001b[0m: Cannot handle this data type: (1, 1, 3), <f8"
     ]
    }
   ],
   "source": [
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "def open_image(path):\n",
    "    image = Image.open(path)\n",
    "    image = transforms.ToTensor()(image)\n",
    "    return image\n",
    "def image_noiser(image):\n",
    "    image = image + torch.randn(image.shape).to(device)*0.1\n",
    "    return image\n",
    "def save_image(image,path):\n",
    "    image = image.cpu()\n",
    "    image = image.detach().numpy()\n",
    "    image = image.transpose(1,2,0)\n",
    "    image = (image*255).astype(np.uint8)\n",
    "    image = Image.fromarray(image)\n",
    "    image.save(path)\n",
    "    return image\n",
    "def get_all_images_from_folder(path):\n",
    "    images = []\n",
    "    for filename in os.listdir(path):\n",
    "        images.append(filename)\n",
    "    return images\n",
    "#Noisy\n",
    "# for images in get_all_images_from_folder(\"D:/Valentin/Desktop/Images ummt/raw/\"):\n",
    "#     save_image(image_noiser(open_image(\"D:/Valentin/Desktop/Images ummt/raw/\"+images)),\"D:/Valentin/Desktop/Images ummt/noisy/\"+images)\n",
    "#Wrong\n",
    "# folder = get_all_images_from_folder(\"D:/Valentin/Desktop/Images ummt/raw/\")\n",
    "# for i in range(len(folder)):\n",
    "#     save_image(open_image(\"D:/Valentin/Desktop/Images ummt/raw/\"+folder[i]),\"D:/Valentin/Desktop/Images ummt/wrong/\"+folder[(i+1)%len(folder)])\n",
    "#Blur\n",
    "!pip install opencv-python\n",
    "import cv2\n",
    "def blur_image(image):\n",
    "    image = image.cpu()\n",
    "    image = image.detach().numpy()\n",
    "    image = image.transpose(1,2,0)\n",
    "    image = (image*255).astype(np.uint8)\n",
    "    image = cv2.blur(image,(5,5))\n",
    "    image = Image.fromarray(image)\n",
    "    image = transforms.ToTensor()(image)\n",
    "    return image\n",
    "for images in get_all_images_from_folder(\"D:/Valentin/Desktop/Images ummt/raw/\"):\n",
    "    save_image(blur_image(open_image(\"D:/Valentin/Desktop/Images ummt/raw/\"+images)),\"D:/Valentin/Desktop/Images ummt/blurred/\"+images)\n",
    "#Negative\n",
    "def negative_image(image):\n",
    "    image = image.cpu()\n",
    "    image = image.detach().numpy()\n",
    "    image = image.transpose(1,2,0)\n",
    "    image = (image*255).astype(np.uint8)\n",
    "    image = 255-image\n",
    "    image = Image.fromarray(image)\n",
    "    image = transforms.ToTensor()(image)\n",
    "    return image\n",
    "for images in get_all_images_from_folder(\"D:/Valentin/Desktop/Images ummt/raw/\"):   \n",
    "    save_image(negative_image(open_image(\"D:/Valentin/Desktop/Images ummt/raw/\"+images)),\"D:/Valentin/Desktop/Images ummt/negative/\"+images)\n",
    "#Rotation\n",
    "def rotate_image(image):\n",
    "    image = image.cpu()\n",
    "    image = image.detach().numpy()\n",
    "    image = image.transpose(1,2,0)\n",
    "    image = (image*255).astype(np.uint8)\n",
    "    image = cv2.rotate(image,cv2.ROTATE_90_CLOCKWISE)\n",
    "    image = Image.fromarray(image)\n",
    "    image = transforms.ToTensor()(image)\n",
    "    return image\n",
    "for images in get_all_images_from_folder(\"D:/Valentin/Desktop/Images ummt/raw/\"):\n",
    "    save_image(rotate_image(open_image(\"D:/Valentin/Desktop/Images ummt/raw/\"+images)),\"D:/Valentin/Desktop/Images ummt/rotated/\"+images)\n",
    "#Contrast\n",
    "def contrast_image(image):\n",
    "    image = image.cpu()\n",
    "    image = image.detach().numpy()\n",
    "    image = image.transpose(1,2,0)\n",
    "    image = (image*255).astype(np.uint8)\n",
    "    # image = cv2.equalizeHist(image)\n",
    "    image= cv2.convertScaleAbs(image, 10, 2)\n",
    "\n",
    "    image = Image.fromarray(image)\n",
    "    image = transforms.ToTensor()(image)\n",
    "    return image\n",
    "for images in get_all_images_from_folder(\"D:/Valentin/Desktop/Images ummt/raw/\"):\n",
    "    save_image(contrast_image(open_image(\"D:/Valentin/Desktop/Images ummt/raw/\"+images)),\"D:/Valentin/Desktop/Images ummt/contrast/\"+images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "dde2f4f8",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'gate.'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[96], line 29\u001b[0m\n\u001b[0;32m     27\u001b[0m longueur_max \u001b[39m=\u001b[39m \u001b[39m64\u001b[39m\n\u001b[0;32m     28\u001b[0m text \u001b[39m=\u001b[39m [[phrase[i] \u001b[39mif\u001b[39;00m i \u001b[39m<\u001b[39m \u001b[39mlen\u001b[39m(phrase) \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mTOKEN_VIDE\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m (longueur_max)] \u001b[39mfor\u001b[39;00m phrase \u001b[39min\u001b[39;00m text]\n\u001b[1;32m---> 29\u001b[0m text \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor([[vocab_en[mot] \u001b[39mfor\u001b[39;00m mot \u001b[39min\u001b[39;00m phrase] \u001b[39mfor\u001b[39;00m phrase \u001b[39min\u001b[39;00m text])\u001b[39m.\u001b[39mto(device \u001b[39m=\u001b[39m device, dtype \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mlong)\n",
      "Cell \u001b[1;32mIn[96], line 29\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     27\u001b[0m longueur_max \u001b[39m=\u001b[39m \u001b[39m64\u001b[39m\n\u001b[0;32m     28\u001b[0m text \u001b[39m=\u001b[39m [[phrase[i] \u001b[39mif\u001b[39;00m i \u001b[39m<\u001b[39m \u001b[39mlen\u001b[39m(phrase) \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mTOKEN_VIDE\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m (longueur_max)] \u001b[39mfor\u001b[39;00m phrase \u001b[39min\u001b[39;00m text]\n\u001b[1;32m---> 29\u001b[0m text \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor([[vocab_en[mot] \u001b[39mfor\u001b[39;00m mot \u001b[39min\u001b[39;00m phrase] \u001b[39mfor\u001b[39;00m phrase \u001b[39min\u001b[39;00m text])\u001b[39m.\u001b[39mto(device \u001b[39m=\u001b[39m device, dtype \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mlong)\n",
      "Cell \u001b[1;32mIn[96], line 29\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     27\u001b[0m longueur_max \u001b[39m=\u001b[39m \u001b[39m64\u001b[39m\n\u001b[0;32m     28\u001b[0m text \u001b[39m=\u001b[39m [[phrase[i] \u001b[39mif\u001b[39;00m i \u001b[39m<\u001b[39m \u001b[39mlen\u001b[39m(phrase) \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mTOKEN_VIDE\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m (longueur_max)] \u001b[39mfor\u001b[39;00m phrase \u001b[39min\u001b[39;00m text]\n\u001b[1;32m---> 29\u001b[0m text \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor([[vocab_en[mot] \u001b[39mfor\u001b[39;00m mot \u001b[39min\u001b[39;00m phrase] \u001b[39mfor\u001b[39;00m phrase \u001b[39min\u001b[39;00m text])\u001b[39m.\u001b[39mto(device \u001b[39m=\u001b[39m device, dtype \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mlong)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'gate.'"
     ]
    }
   ],
   "source": [
    "with open('Images ummt/captions.txt','r') as file : \n",
    "    lines = file.readlines()\n",
    "    file.close()\n",
    "def get_resnet_features_from_image(image):\n",
    "    resnet = models.resnet152(pretrained=True)\n",
    "    resnet.eval()\n",
    "    resnet = resnet.to(device)\n",
    "    resnet = nn.Sequential(*list(resnet.children())[:-1])\n",
    "    image = image.to(device)\n",
    "    image = image.unsqueeze(0)\n",
    "    image = resnet(image)\n",
    "    image = image.squeeze(0)\n",
    "    return image\n",
    "\n",
    "lines = [line[:-1].split(',') for line in  lines]\n",
    "images= [line[0] for line in lines]\n",
    "captions = [line[1][1:] for line in lines]\n",
    "folder =[x for x in os.listdir('Images ummt') if x != 'captions.txt']\n",
    "text=[]\n",
    "features=[]\n",
    "for name in folder :\n",
    "    text+=captions\n",
    "    # for image in images: \n",
    "    #     features.append(get_resnet_features_from_image(open_image('Images ummt/'+name+'/'+image)))\n",
    "text = [[\"DEBUT_DE_PHRASE\"]+ligne.strip().split(\" \")+[\"FIN_DE_PHRASE\"] for ligne in text ]\n",
    "longueur_max = 64\n",
    "text = [[phrase[i] if i < len(phrase) else \"TOKEN_VIDE\" for i in range (longueur_max)] for phrase in text]\n",
    "text = torch.tensor([[vocab_en[mot] for mot in phrase] for phrase in text]).to(device = device, dtype = torch.long)\n",
    "text = torch.cat((text, torch.zeros(batch_size-text.shape[0]).to(device = device, dtype = torch.long)))\n",
    "features = torch.cat(features,torch.zeros(batch_size-features.shape[0],features.shape[1],features.shape[2],features.shape[2]).to(device = device, dtype = torch.float))\n",
    "#Pour la suite : faire un machin qui fait un dataframe avec la traduction, la source, l'image, le nom de la transformation, le bleu score, l'image source, et l'attention image et text\n",
    "#Essayer de modifier l'output du decoder pour faire en sorte qu'il output aussi l'attention\n",
    "#se baser sur le programme traduction dans evaluateur et ça devrait aller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "59167376",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.zeros(120)\n",
    "a"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
