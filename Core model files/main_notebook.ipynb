{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "efb10bae-68df-4d91-9e10-2cecaf169e71",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\valen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# !git clone https://github.com/tordjx/ummt.git\n",
    "# import os \n",
    "# os.chdir(\"ummt/Core model files\")\n",
    "# os.getcwd()\n",
    "!pip install matplotlib --quiet\n",
    "!pip install livelossplot --quiet\n",
    "!pip install nltk --quiet\n",
    "!pip install torchtext --quiet\n",
    "#mc cp s3/tordjx ummt/Core*model*files --recursive\n",
    "# watch -n 0.5 nvidia-smi\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "image_bool = False\n",
    "load_model = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f7ee3bb-7c9c-47e3-ae7f-064be78e7002",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#%%\n",
    "from Modele_decodeur_maison import *\n",
    "from Pipeline import *\n",
    "from Trainer import * \n",
    "from greedy_beam_search import *\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "batch_size = 256\n",
    "\n",
    "# Texts\n",
    "tokenized_fr,tokenized_en, vocab_fr,vocab_en = get_train_data_nouveau(batch_size,True)\n",
    "tokenized_val_fr,tokenized_val_en, _,_ = get_train_data_nouveau(batch_size,False)\n",
    "#Data non batchés\n",
    "n_token_fr = len(vocab_fr.keys())\n",
    "n_token_en = len(vocab_en.keys())\n",
    "\n",
    "inv_map_en = {v: k for k, v in vocab_en.items()}\n",
    "inv_map_fr = {v: k for k, v in vocab_fr.items()}\n",
    "\n",
    "n_head =8\n",
    "num_encoder_layers = 4\n",
    "num_decoder_layers = 4\n",
    "dim_feedforward = 512\n",
    "dropout = 0.1\n",
    "activation = nn.Softmax(dim=2)\n",
    "embedding_dim = 128\n",
    "\n",
    "model_fr = Modèle(n_token_fr,embedding_dim,n_head, num_encoder_layers,num_decoder_layers,dim_feedforward,dropout,activation,vocab_fr[\"TOKEN_VIDE\"],vocab_fr[\"DEBUT_DE_PHRASE\"],vocab_fr[\"FIN_DE_PHRASE\"],False,'').to(device)\n",
    "model_en = Modèle(n_token_en,embedding_dim,n_head, num_encoder_layers,num_decoder_layers,dim_feedforward,dropout,activation,vocab_en[\"TOKEN_VIDE\"],vocab_en[\"DEBUT_DE_PHRASE\"],vocab_en[\"FIN_DE_PHRASE\"],False,'').to(device)\n",
    "model_fr.feedforward= model_en.feedforward\n",
    "for i in range(3):\n",
    "  model_fr.decoder.layers[i]= model_en.decoder.layers[i]\n",
    "  model_fr.encoder.layers[i]= model_en.encoder.layers[i]\n",
    "prefix = \"fulltry\"\n",
    "if load_model : \n",
    "    model_en.load_state_dict(torch.load(\"tordjx/\"+prefix+\"_en\"))\n",
    "    model_fr.load_state_dict(torch.load(\"tordjx/\"+prefix+\"_fr\"))\n",
    "else : \n",
    "    with open(\"logs.txt\",'w') as logs :\n",
    "        logs.write(\"\")\n",
    "        logs.close()\n",
    "if image_bool :\n",
    "    train_features  = np.load(\"tordjx/train-resnet50-res4frelu.npy\")\n",
    "    val_features = np.load(\"tordjx/val-resnet50-res4frelu.npy\")\n",
    "    train_features = torch.from_numpy(train_features)\n",
    "    val_features = torch.from_numpy(val_features)\n",
    "    train_data_fr = [tokenized_fr, train_features]\n",
    "    train_data_en = [tokenized_en, train_features]\n",
    "    val_data_fr = [tokenized_val_fr, val_features]\n",
    "    val_data_en = [tokenized_val_en, val_features]\n",
    "def save_model(model_en,model_fr,prefix):\n",
    "    torch.save(model_fr.state_dict(), \"tordjx/\"+prefix+\"_fr\")\n",
    "    torch.save(model_en.state_dict(), \"tordjx/\"+prefix+\"_en\")\n",
    "    import os\n",
    "    import s3fs\n",
    "    !pip install pandas\n",
    "    import pandas\n",
    "    # Create filesystem object\n",
    "    S3_ENDPOINT_URL = \"https://\" + os.environ[\"AWS_S3_ENDPOINT\"]\n",
    "    fs = s3fs.S3FileSystem(client_kwargs={'endpoint_url': S3_ENDPOINT_URL})\n",
    "    fs.upload(\"tordjx/\"+prefix+\"_fr\",\"tordjx/\"+prefix+\"_fr\")\n",
    "    fs.upload(\"tordjx/\"+prefix+\"_en\",\"tordjx/\"+prefix+\"_en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d6a5f97-980c-4051-a19b-422479724a88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Programmes\\Python\\lib\\site-packages\\torch\\nn\\functional.py:4999: UserWarning: Support for mismatched src_key_padding_mask and src_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n",
      "d:\\Programmes\\Python\\lib\\site-packages\\torch\\nn\\functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'batch_size' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 18\u001b[0m\n\u001b[0;32m     16\u001b[0m     mixed_train(val_data_en,val_data_fr,inv_map_en,inv_map_fr,model_fr,model_en,train_data_fr,train_data_en,\u001b[39m60\u001b[39m,batch_size, \u001b[39mTrue\u001b[39;00m,\u001b[39m1\u001b[39m\u001b[39m/\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[0;32m     17\u001b[0m \u001b[39melse\u001b[39;00m :\n\u001b[1;32m---> 18\u001b[0m     mixed_train(tokenized_val_en,tokenized_val_fr,inv_map_en,inv_map_fr,model_fr,model_en,tokenized_fr,tokenized_en,\u001b[39m60\u001b[39;49m,batch_size, \u001b[39mFalse\u001b[39;49;00m,\u001b[39m1\u001b[39;49m\u001b[39m/\u001b[39;49m\u001b[39m2\u001b[39;49m)\n\u001b[0;32m     19\u001b[0m torch\u001b[39m.\u001b[39msave(model_fr\u001b[39m.\u001b[39mstate_dict(), \u001b[39m\"\u001b[39m\u001b[39mtordjx/\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m+\u001b[39mprefix\u001b[39m+\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m_fr\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     20\u001b[0m torch\u001b[39m.\u001b[39msave(model_en\u001b[39m.\u001b[39mstate_dict(), \u001b[39m\"\u001b[39m\u001b[39mtordjx/\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m+\u001b[39mprefix\u001b[39m+\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m_en\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32md:\\Valentin\\Documents\\ENSAE\\STATAPP\\UMMT\\Core model files\\Trainer.py:99\u001b[0m, in \u001b[0;36mmixed_train\u001b[1;34m(val_data_en, val_data_fr, inv_map_en, inv_map_fr, model_fr, model_en, train_data_fr, train_data_en, n_iter, batch_size, image_bool, part_auto_encoding)\u001b[0m\n\u001b[0;32m     97\u001b[0m tokenized_val_fr \u001b[39m=\u001b[39m val_data_fr[\u001b[39m0\u001b[39m]\n\u001b[0;32m     98\u001b[0m \u001b[39mfor\u001b[39;00m i_iter \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_iter):\n\u001b[1;32m---> 99\u001b[0m     save_dataframe_eval(model_fr,model_en,tokenized_val_en,tokenized_val_fr,inv_map_en,inv_map_fr,image_bool,i_iter)\n\u001b[0;32m    100\u001b[0m     model_en\u001b[39m.\u001b[39mcurr_epoch \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m\n\u001b[0;32m    101\u001b[0m     model_fr\u001b[39m.\u001b[39mcurr_epoch \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m\n",
      "File \u001b[1;32md:\\Valentin\\Documents\\ENSAE\\STATAPP\\UMMT\\Core model files\\evaluateur.py:197\u001b[0m, in \u001b[0;36msave_dataframe_eval\u001b[1;34m(model_fr, model_en, tokenized_val_en, tokenized_val_fr, inv_map_en, inv_map_fr, image_bool, epoch)\u001b[0m\n\u001b[0;32m    196\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msave_dataframe_eval\u001b[39m(model_fr,model_en,tokenized_val_en,tokenized_val_fr,inv_map_en,inv_map_fr,image_bool,epoch \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m):\n\u001b[1;32m--> 197\u001b[0m     df \u001b[39m=\u001b[39m dataframe_eval(model_fr,model_en,tokenized_val_en,tokenized_val_fr,inv_map_en,inv_map_fr,image_bool)\n\u001b[0;32m    198\u001b[0m     df[\u001b[39m\"\u001b[39m\u001b[39mbleu_en_fr\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m row: bleu(row,\u001b[39m\"\u001b[39m\u001b[39men\u001b[39m\u001b[39m\"\u001b[39m), axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m    199\u001b[0m     df[\u001b[39m\"\u001b[39m\u001b[39mbleu_fr_en\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m row: bleu(row,\u001b[39m\"\u001b[39m\u001b[39mfr\u001b[39m\u001b[39m\"\u001b[39m), axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32md:\\Valentin\\Documents\\ENSAE\\STATAPP\\UMMT\\Core model files\\evaluateur.py:127\u001b[0m, in \u001b[0;36mdataframe_eval\u001b[1;34m(model_fr, model_en, val_data_en, val_data_fr, inv_map_en, inv_map_fr, image_bool)\u001b[0m\n\u001b[0;32m    125\u001b[0m \u001b[39melse\u001b[39;00m :\n\u001b[0;32m    126\u001b[0m     val_data_en,val_data_fr \u001b[39m=\u001b[39m val_data_en,val_data_fr\n\u001b[1;32m--> 127\u001b[0m batched_data_en,batched_data_fr\u001b[39m=\u001b[39mbatchify([val_data_en,val_data_fr],batch_size,image_bool,conservative\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,permute \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m    128\u001b[0m \u001b[39mif\u001b[39;00m image_bool:\n\u001b[0;32m    129\u001b[0m     src,features \u001b[39m=\u001b[39m batched_data_en\n",
      "\u001b[1;31mNameError\u001b[0m: name 'batch_size' is not defined"
     ]
    }
   ],
   "source": [
    "#On va entrainer 4 modeles : avec/sans images, avec/sans teacher forcing\n",
    "teacher_forcing = False\n",
    "bools = [True,False]\n",
    "for image_bool in bools : \n",
    "    prefix = str(image_bool)+str(teacher_forcing)\n",
    "    model_fr = Modèle(n_token_fr,embedding_dim,n_head, num_encoder_layers,num_decoder_layers,dim_feedforward,dropout,activation,vocab_fr[\"TOKEN_VIDE\"],vocab_fr[\"DEBUT_DE_PHRASE\"],vocab_fr[\"FIN_DE_PHRASE\"],teacher_forcing,prefix).to(device)\n",
    "    model_en = Modèle(n_token_en,embedding_dim,n_head, num_encoder_layers,num_decoder_layers,dim_feedforward,dropout,activation,vocab_en[\"TOKEN_VIDE\"],vocab_en[\"DEBUT_DE_PHRASE\"],vocab_en[\"FIN_DE_PHRASE\"],teacher_forcing,prefix).to(device)\n",
    "    model_fr.feedforward= model_en.feedforward\n",
    "    for i in range(3):\n",
    "        model_fr.decoder.layers[i]= model_en.decoder.layers[i]\n",
    "        model_fr.encoder.layers[i]= model_en.encoder.layers[i]\n",
    "    with open(prefix+\"logs.txt\",'w') as logs :\n",
    "        logs.write(\"\")\n",
    "        logs.close()\n",
    "    if image_bool :\n",
    "        mixed_train(val_data_en,val_data_fr,inv_map_en,inv_map_fr,model_fr,model_en,train_data_fr,train_data_en,60,batch_size, True,1/2)\n",
    "    else :\n",
    "        mixed_train(tokenized_val_en,tokenized_val_fr,inv_map_en,inv_map_fr,model_fr,model_en,tokenized_fr,tokenized_en,60,batch_size, False,1/2)\n",
    "    torch.save(model_fr.state_dict(), \"tordjx/\"+prefix+\"_fr\")\n",
    "    torch.save(model_en.state_dict(), \"tordjx/\"+prefix+\"_en\")\n",
    "    pd.DataFrame(model_en.loss_list).to_csv(\"tordjx/\"+prefix+\"loss_en.csv\")\n",
    "    pd.DataFrame(model_fr.loss_list).to_csv(\"tordjx/\"+prefix+\"loss_fr.csv\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba8bf1c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Programmes\\Python\\lib\\site-packages\\torch\\nn\\functional.py:4999: UserWarning: Support for mismatched src_key_padding_mask and src_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n",
      "d:\\Programmes\\Python\\lib\\site-packages\\torch\\nn\\functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.7672,  0.6324, -2.2318,  ...,  3.2074, -1.5971,  0.5021],\n",
       "         [-1.8711, -1.1175, -1.7387,  ...,  1.9533, -0.6104,  0.6691],\n",
       "         [-2.5070,  0.7686, -1.9276,  ...,  2.5660, -0.9294,  0.5455],\n",
       "         ...,\n",
       "         [-2.2627,  0.6238, -1.7786,  ...,  2.0241, -1.1009,  1.6198],\n",
       "         [-1.3301,  0.1612, -1.9499,  ...,  1.6829, -1.0265,  1.4370],\n",
       "         [-1.4545,  0.3633, -1.3041,  ...,  1.8026, -1.3168,  1.2916]],\n",
       "\n",
       "        [[ 0.4346, -1.8247, -2.3207,  ...,  2.7586, -0.3141, -0.0385],\n",
       "         [-0.2487, -0.9901, -1.9830,  ...,  2.4211, -0.4552,  0.7975],\n",
       "         [-0.5314, -0.5862, -1.6345,  ...,  2.3901, -0.4316,  0.7297],\n",
       "         ...,\n",
       "         [-1.5806, -0.3029, -1.5977,  ...,  3.0642, -1.3033,  0.8397],\n",
       "         [-1.0783, -0.6023, -2.4334,  ...,  1.8654, -0.2117,  0.9085],\n",
       "         [-1.1355, -0.4256, -1.5829,  ...,  3.0951, -1.1788,  1.3038]],\n",
       "\n",
       "        [[-0.4734, -1.1233, -2.7201,  ...,  3.4664,  0.1761,  0.8877],\n",
       "         [-1.1830, -1.4720, -1.6237,  ...,  2.1842, -0.6953, -0.3005],\n",
       "         [-1.6022, -1.3293, -1.7169,  ...,  1.5716, -0.1938,  0.9530],\n",
       "         ...,\n",
       "         [-2.2047, -1.1574, -1.9742,  ...,  2.6724,  0.5779,  1.2782],\n",
       "         [-1.8340, -0.8617, -1.7533,  ...,  2.0062,  0.3763,  0.3884],\n",
       "         [-1.8117, -0.9105, -1.2268,  ...,  1.7089, -0.2627,  0.6535]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-1.7445, -1.0802, -3.8266,  ...,  2.8141, -0.1897,  0.1665],\n",
       "         [-1.4566, -1.8985, -3.2853,  ...,  2.9236, -0.1663, -0.1350],\n",
       "         [-0.9470, -1.6510, -3.5528,  ...,  2.8092, -0.2734,  0.0116],\n",
       "         ...,\n",
       "         [ 0.4313, -1.4283, -2.5295,  ...,  2.1533, -0.5493,  0.8083],\n",
       "         [ 0.5044, -0.7629, -2.9392,  ...,  1.6745,  0.2206,  0.4683],\n",
       "         [-0.3311, -1.0687, -3.0374,  ...,  1.7406,  0.0568,  0.4826]],\n",
       "\n",
       "        [[-0.7750, -1.4568, -3.1031,  ...,  2.6716, -0.4005,  0.4410],\n",
       "         [ 0.3063, -1.9225, -2.6522,  ...,  3.1562, -0.5556,  0.4125],\n",
       "         [-0.4148, -1.9725, -2.8228,  ...,  2.2459, -0.0563,  0.9737],\n",
       "         ...,\n",
       "         [-0.6264, -0.7993, -2.9419,  ...,  2.6457,  0.2646,  1.3802],\n",
       "         [-0.4947, -1.6511, -3.1860,  ...,  2.0097,  0.7460,  1.2256],\n",
       "         [-0.5192, -1.0773, -2.6038,  ...,  2.2966,  0.0607,  1.2168]],\n",
       "\n",
       "        [[ 0.0483, -1.3987, -3.2099,  ...,  1.8952, -0.8749, -0.2009],\n",
       "         [ 0.3864, -1.3319, -2.1818,  ...,  2.6513, -0.0176, -0.2622],\n",
       "         [-0.2619, -1.4960, -3.2397,  ...,  2.8554,  1.0315,  0.2707],\n",
       "         ...,\n",
       "         [-0.2402, -1.4280, -1.8668,  ...,  1.6698,  0.7460,  0.8663],\n",
       "         [-0.9940, -1.3050, -1.7044,  ...,  1.9627,  0.7954,  1.1541],\n",
       "         [-0.7783, -1.0880, -1.4609,  ...,  1.9946,  0.3037,  1.6723]]],\n",
       "       grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_en(tokenized_val_en[:batch_size],False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09bd9a6-d152-4dfa-984f-f9393a53199b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# if image_bool :\n",
    "#     loss_list = learning_rate_finder(val_data_en,val_data_fr,inv_map_en,inv_map_fr,model_fr,model_en,train_data_fr,train_data_en,150,batch_size, True,1/2)\n",
    "# else :\n",
    "#     loss_list = learning_rate_finder(tokenized_val_en,tokenized_val_fr,inv_map_en,inv_map_fr,model_fr,model_en,tokenized_fr,tokenized_en,150,batch_size, False,1/2)\n",
    "\n",
    "lrs = [10**(-10)*3**n for n in range(len(loss_list))]\n",
    "cut = 11\n",
    "begin = 10\n",
    "plt.plot(lrs[begin:-cut],loss_list[begin:-cut])\n",
    "plt.xscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7312e76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Programmes\\Python\\lib\\site-packages\\torch\\nn\\functional.py:4999: UserWarning: Support for mismatched src_key_padding_mask and src_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n",
      "d:\\Programmes\\Python\\lib\\site-packages\\torch\\nn\\functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 99\u001b[0m\n\u001b[0;32m     96\u001b[0m         df[\u001b[39m\"\u001b[39m\u001b[39mbleu_fr_en_txt_only\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m row: bleu_txt_only(row,\u001b[39m\"\u001b[39m\u001b[39mfr\u001b[39m\u001b[39m\"\u001b[39m), axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m     97\u001b[0m     df\u001b[39m.\u001b[39mto_csv(model_fr\u001b[39m.\u001b[39mprefix\u001b[39m+\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m_eval.csv\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 99\u001b[0m dataframe_eval(model_fr,model_en,tokenized_val_en,tokenized_val_fr,inv_map_en,inv_map_fr,image_bool)\n",
      "Cell \u001b[1;32mIn[4], line 54\u001b[0m, in \u001b[0;36mdataframe_eval\u001b[1;34m(model_fr, model_en, val_data_en, val_data_fr, inv_map_en, inv_map_fr, image_bool)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[39melse\u001b[39;00m :\n\u001b[0;32m     53\u001b[0m     src[batch] ,tgt[batch]\u001b[39m=\u001b[39m src[batch]\u001b[39m.\u001b[39mto(device),tgt[batch]\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m---> 54\u001b[0m     traduction \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39margmax(CCF_greedy(model_en,model_fr,src[batch],\u001b[39mNone\u001b[39;49;00m,image_bool) ,dim \u001b[39m=\u001b[39m \u001b[39m2\u001b[39m)\n\u001b[0;32m     56\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(traduction\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]):\n\u001b[0;32m     57\u001b[0m     traductions_en_fr\u001b[39m.\u001b[39mappend(inv_map_fr[traduction[i][j]\u001b[39m.\u001b[39mitem()]  \u001b[39mfor\u001b[39;00m j \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(traduction\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]) \u001b[39mif\u001b[39;00m inv_map_fr[traduction[i][j]\u001b[39m.\u001b[39mitem()] \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m [\u001b[39m\"\u001b[39m\u001b[39mTOKEN_VIDE\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39mDEBUT_DE_PHRASE\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39mFIN_DE_PHRASE\u001b[39m\u001b[39m\"\u001b[39m])\n",
      "File \u001b[1;32md:\\Valentin\\Documents\\ENSAE\\STATAPP\\UMMT\\Core model files\\greedy_beam_search.py:57\u001b[0m, in \u001b[0;36mCCF_greedy\u001b[1;34m(model_A, model_B, text_input, image_input, image_bool)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     55\u001b[0m     x \u001b[39m=\u001b[39m text_encoded\n\u001b[1;32m---> 57\u001b[0m     output \u001b[39m=\u001b[39m model_B\u001b[39m.\u001b[39;49mdecoder(model_B\u001b[39m.\u001b[39;49mpositional_encoder(model_B\u001b[39m.\u001b[39;49membedding(decoder_input\u001b[39m.\u001b[39;49mto(device))),x, tgt_mask , [memory_mask] , tgt_padding_mask, [memory_key_padding_mask])\n\u001b[0;32m     59\u001b[0m \u001b[39m# Greedy \u001b[39;00m\n\u001b[0;32m     60\u001b[0m prob \u001b[39m=\u001b[39m  model_B\u001b[39m.\u001b[39moutput_layer(output)\n",
      "File \u001b[1;32md:\\Programmes\\Python\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32md:\\Programmes\\Python\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:360\u001b[0m, in \u001b[0;36mTransformerDecoder.forward\u001b[1;34m(self, tgt, memory, tgt_mask, memory_mask, tgt_key_padding_mask, memory_key_padding_mask)\u001b[0m\n\u001b[0;32m    357\u001b[0m output \u001b[39m=\u001b[39m tgt\n\u001b[0;32m    359\u001b[0m \u001b[39mfor\u001b[39;00m mod \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers:\n\u001b[1;32m--> 360\u001b[0m     output \u001b[39m=\u001b[39m mod(output, memory, tgt_mask\u001b[39m=\u001b[39;49mtgt_mask,\n\u001b[0;32m    361\u001b[0m                  memory_mask\u001b[39m=\u001b[39;49mmemory_mask,\n\u001b[0;32m    362\u001b[0m                  tgt_key_padding_mask\u001b[39m=\u001b[39;49mtgt_key_padding_mask,\n\u001b[0;32m    363\u001b[0m                  memory_key_padding_mask\u001b[39m=\u001b[39;49mmemory_key_padding_mask)\n\u001b[0;32m    365\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    366\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm(output)\n",
      "File \u001b[1;32md:\\Programmes\\Python\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32md:\\Valentin\\Documents\\ENSAE\\STATAPP\\UMMT\\Core model files\\NewDecoderLayer.py:80\u001b[0m, in \u001b[0;36mTransformerDecoderLayer.forward\u001b[1;34m(self, x, memory, tgt_mask, memory_mask, tgt_key_padding_mask, memory_key_padding_mask)\u001b[0m\n\u001b[0;32m     78\u001b[0m i_outputs \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     79\u001b[0m x2 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm_1(text)\n\u001b[1;32m---> 80\u001b[0m x \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout_1(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattn_1(x2, x2, x2, key_padding_mask\u001b[39m=\u001b[39;49mtgt_key_padding_mask, attn_mask\u001b[39m=\u001b[39;49mtgt_mask)[\u001b[39m0\u001b[39m])\n\u001b[0;32m     81\u001b[0m \u001b[39m# Here, att1 returns a tuple, the first being the result, the second being the attention weights\u001b[39;00m\n\u001b[0;32m     82\u001b[0m x2 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm_2(x)\n",
      "File \u001b[1;32md:\\Programmes\\Python\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32md:\\Programmes\\Python\\lib\\site-packages\\torch\\nn\\modules\\activation.py:1189\u001b[0m, in \u001b[0;36mMultiheadAttention.forward\u001b[1;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights, is_causal)\u001b[0m\n\u001b[0;32m   1175\u001b[0m     attn_output, attn_output_weights \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mmulti_head_attention_forward(\n\u001b[0;32m   1176\u001b[0m         query, key, value, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed_dim, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_heads,\n\u001b[0;32m   1177\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39min_proj_weight, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39min_proj_bias,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1186\u001b[0m         average_attn_weights\u001b[39m=\u001b[39maverage_attn_weights,\n\u001b[0;32m   1187\u001b[0m         is_causal\u001b[39m=\u001b[39mis_causal)\n\u001b[0;32m   1188\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1189\u001b[0m     attn_output, attn_output_weights \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39;49mmulti_head_attention_forward(\n\u001b[0;32m   1190\u001b[0m         query, key, value, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membed_dim, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_heads,\n\u001b[0;32m   1191\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49min_proj_weight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49min_proj_bias,\n\u001b[0;32m   1192\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias_k, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias_v, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madd_zero_attn,\n\u001b[0;32m   1193\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropout, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mout_proj\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mout_proj\u001b[39m.\u001b[39;49mbias,\n\u001b[0;32m   1194\u001b[0m         training\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining,\n\u001b[0;32m   1195\u001b[0m         key_padding_mask\u001b[39m=\u001b[39;49mkey_padding_mask,\n\u001b[0;32m   1196\u001b[0m         need_weights\u001b[39m=\u001b[39;49mneed_weights,\n\u001b[0;32m   1197\u001b[0m         attn_mask\u001b[39m=\u001b[39;49mattn_mask,\n\u001b[0;32m   1198\u001b[0m         average_attn_weights\u001b[39m=\u001b[39;49maverage_attn_weights,\n\u001b[0;32m   1199\u001b[0m         is_causal\u001b[39m=\u001b[39;49mis_causal)\n\u001b[0;32m   1200\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_first \u001b[39mand\u001b[39;00m is_batched:\n\u001b[0;32m   1201\u001b[0m     \u001b[39mreturn\u001b[39;00m attn_output\u001b[39m.\u001b[39mtranspose(\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m), attn_output_weights\n",
      "File \u001b[1;32md:\\Programmes\\Python\\lib\\site-packages\\torch\\nn\\functional.py:5188\u001b[0m, in \u001b[0;36mmulti_head_attention_forward\u001b[1;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v, average_attn_weights, is_causal)\u001b[0m\n\u001b[0;32m   5186\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m use_separate_proj_weight:\n\u001b[0;32m   5187\u001b[0m     \u001b[39massert\u001b[39;00m in_proj_weight \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m, \u001b[39m\"\u001b[39m\u001b[39muse_separate_proj_weight is False but in_proj_weight is None\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m-> 5188\u001b[0m     q, k, v \u001b[39m=\u001b[39m _in_projection_packed(query, key, value, in_proj_weight, in_proj_bias)\n\u001b[0;32m   5189\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   5190\u001b[0m     \u001b[39massert\u001b[39;00m q_proj_weight \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m, \u001b[39m\"\u001b[39m\u001b[39muse_separate_proj_weight is True but q_proj_weight is None\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[1;32md:\\Programmes\\Python\\lib\\site-packages\\torch\\nn\\functional.py:4765\u001b[0m, in \u001b[0;36m_in_projection_packed\u001b[1;34m(q, k, v, w, b)\u001b[0m\n\u001b[0;32m   4762\u001b[0m \u001b[39mif\u001b[39;00m k \u001b[39mis\u001b[39;00m v:\n\u001b[0;32m   4763\u001b[0m     \u001b[39mif\u001b[39;00m q \u001b[39mis\u001b[39;00m k:\n\u001b[0;32m   4764\u001b[0m         \u001b[39m# self-attention\u001b[39;00m\n\u001b[1;32m-> 4765\u001b[0m         proj \u001b[39m=\u001b[39m linear(q, w, b)\n\u001b[0;32m   4766\u001b[0m         \u001b[39m# reshape to 3, E and not E, 3 is deliberate for better memory coalescing and keeping same order as chunk()\u001b[39;00m\n\u001b[0;32m   4767\u001b[0m         proj \u001b[39m=\u001b[39m proj\u001b[39m.\u001b[39munflatten(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, (\u001b[39m3\u001b[39m, E))\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\u001b[39m.\u001b[39mtranspose(\u001b[39m0\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m2\u001b[39m)\u001b[39m.\u001b[39msqueeze(\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m)\u001b[39m.\u001b[39mcontiguous()\n",
      "File \u001b[1;32md:\\Programmes\\Python\\lib\\site-packages\\torch\\fx\\traceback.py:41\u001b[0m, in \u001b[0;36mformat_stack\u001b[1;34m()\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[39mreturn\u001b[39;00m [current_meta\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mstack_trace\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m)]\n\u001b[0;32m     39\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     40\u001b[0m     \u001b[39m# fallback to traceback.format_stack()\u001b[39;00m\n\u001b[1;32m---> 41\u001b[0m     \u001b[39mreturn\u001b[39;00m traceback\u001b[39m.\u001b[39mformat_list(traceback\u001b[39m.\u001b[39;49mextract_stack()[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\n",
      "File \u001b[1;32md:\\Programmes\\Python\\lib\\traceback.py:227\u001b[0m, in \u001b[0;36mextract_stack\u001b[1;34m(f, limit)\u001b[0m\n\u001b[0;32m    225\u001b[0m \u001b[39mif\u001b[39;00m f \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    226\u001b[0m     f \u001b[39m=\u001b[39m sys\u001b[39m.\u001b[39m_getframe()\u001b[39m.\u001b[39mf_back\n\u001b[1;32m--> 227\u001b[0m stack \u001b[39m=\u001b[39m StackSummary\u001b[39m.\u001b[39;49mextract(walk_stack(f), limit\u001b[39m=\u001b[39;49mlimit)\n\u001b[0;32m    228\u001b[0m stack\u001b[39m.\u001b[39mreverse()\n\u001b[0;32m    229\u001b[0m \u001b[39mreturn\u001b[39;00m stack\n",
      "File \u001b[1;32md:\\Programmes\\Python\\lib\\traceback.py:379\u001b[0m, in \u001b[0;36mStackSummary.extract\u001b[1;34m(klass, frame_gen, limit, lookup_lines, capture_locals)\u001b[0m\n\u001b[0;32m    376\u001b[0m     result\u001b[39m.\u001b[39mappend(FrameSummary(\n\u001b[0;32m    377\u001b[0m         filename, lineno, name, lookup_line\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, \u001b[39mlocals\u001b[39m\u001b[39m=\u001b[39mf_locals))\n\u001b[0;32m    378\u001b[0m \u001b[39mfor\u001b[39;00m filename \u001b[39min\u001b[39;00m fnames:\n\u001b[1;32m--> 379\u001b[0m     linecache\u001b[39m.\u001b[39;49mcheckcache(filename)\n\u001b[0;32m    380\u001b[0m \u001b[39m# If immediate lookup was desired, trigger lookups now.\u001b[39;00m\n\u001b[0;32m    381\u001b[0m \u001b[39mif\u001b[39;00m lookup_lines:\n",
      "File \u001b[1;32md:\\Programmes\\Python\\lib\\linecache.py:72\u001b[0m, in \u001b[0;36mcheckcache\u001b[1;34m(filename)\u001b[0m\n\u001b[0;32m     70\u001b[0m     \u001b[39mcontinue\u001b[39;00m   \u001b[39m# no-op for files loaded via a __loader__\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 72\u001b[0m     stat \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39;49mstat(fullname)\n\u001b[0;32m     73\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mOSError\u001b[39;00m:\n\u001b[0;32m     74\u001b[0m     cache\u001b[39m.\u001b[39mpop(filename, \u001b[39mNone\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Copilot generated code\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "import requests\n",
    "def relu_layer(x):\n",
    "    return torch.nn.functional.relu(x)\n",
    "\n",
    "def open_image(path):\n",
    "    image = Image.open(path)\n",
    "    image = image.resize((224, 224))\n",
    "    image = transforms.ToTensor()(image)\n",
    "    image = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])(image)\n",
    "    return image\n",
    "\n",
    "def get_resnet_features_from_image(image):\n",
    "    resnet = models.resnet152(pretrained=True)\n",
    "    resnet.eval()\n",
    "    resnet = resnet.to(device)\n",
    "    resnet = nn.Sequential(*list(resnet.children())[:-1])\n",
    "    image = image.to(device)\n",
    "    image = image.unsqueeze(0)\n",
    "    image = resnet(image)\n",
    "    image = image.squeeze(0)\n",
    "    return image\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033da8a6-1d98-4eea-be9a-ba67221f6780",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def moving_average(a, n=1,tail = 0) :\n",
    "    ret = np.cumsum(a, dtype=float)\n",
    "    ret[n:] = ret[n:] - ret[:-n]\n",
    "    \n",
    "    return (ret[n - 1:] / n)[tail:]\n",
    "\n",
    "plt.plot(moving_average(model_fr.loss_list))\n",
    "plt.plot(moving_average(model_en.loss_list))\n",
    "print(len(model_fr.loss_list)/(29000/batch_size))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a62a7311-d658-4917-8e00-35d585778982",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenized_val_en = val_data_en[0]\n",
    "i = np.random.randint(len(tokenized_val_en)//batch_size)\n",
    "j = np.random.randint(batch_size)\n",
    "src,features,tgt = donne_random(i,j,val_data_en,val_data_fr,batch_size,True)\n",
    "features = features.to(device,dtype=torch.float32)\n",
    "data = [src,features]\n",
    "# OUT= traduit('greedy',model_en,model_fr,data, inv_map_en,True,tgt[j],inv_map_fr,j)\n",
    "OUT = greedy_beam_search.CCF_greedy(model_en,model_fr,src, features, True)\n",
    "values,indices = torch.topk(OUT,2,2)\n",
    "print(values)\n",
    "print(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5ca3ea82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in d:\\programmes\\python\\lib\\site-packages (4.7.0.72)\n",
      "Requirement already satisfied: numpy>=1.21.2 in d:\\programmes\\python\\lib\\site-packages (from opencv-python) (1.24.2)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Cannot handle this data type: (1, 1, 3), <f8",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32md:\\Programmes\\Python\\lib\\site-packages\\PIL\\Image.py:3080\u001b[0m, in \u001b[0;36mfromarray\u001b[1;34m(obj, mode)\u001b[0m\n\u001b[0;32m   3079\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 3080\u001b[0m     mode, rawmode \u001b[39m=\u001b[39m _fromarray_typemap[typekey]\n\u001b[0;32m   3081\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "\u001b[1;31mKeyError\u001b[0m: ((1, 1, 3), '<f8')",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[57], line 93\u001b[0m\n\u001b[0;32m     91\u001b[0m     \u001b[39mreturn\u001b[39;00m image\n\u001b[0;32m     92\u001b[0m \u001b[39mfor\u001b[39;00m images \u001b[39min\u001b[39;00m get_all_images_from_folder(\u001b[39m\"\u001b[39m\u001b[39mD:/Valentin/Desktop/Images ummt/raw/\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m---> 93\u001b[0m     save_image(blank_image(open_image(\u001b[39m\"\u001b[39;49m\u001b[39mD:/Valentin/Desktop/Images ummt/raw/\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m+\u001b[39;49mimages)),\u001b[39m\"\u001b[39m\u001b[39mD:/Valentin/Desktop/Images ummt/blank/\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m+\u001b[39mimages)\n",
      "Cell \u001b[1;32mIn[57], line 89\u001b[0m, in \u001b[0;36mblank_image\u001b[1;34m(image)\u001b[0m\n\u001b[0;32m     87\u001b[0m image \u001b[39m=\u001b[39m (image\u001b[39m*\u001b[39m\u001b[39m255\u001b[39m)\u001b[39m.\u001b[39mastype(np\u001b[39m.\u001b[39muint8)\n\u001b[0;32m     88\u001b[0m image \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros(image\u001b[39m.\u001b[39mshape)\n\u001b[1;32m---> 89\u001b[0m image \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39;49mfromarray(image)\n\u001b[0;32m     90\u001b[0m image \u001b[39m=\u001b[39m transforms\u001b[39m.\u001b[39mToTensor()(image)\n\u001b[0;32m     91\u001b[0m \u001b[39mreturn\u001b[39;00m image\n",
      "File \u001b[1;32md:\\Programmes\\Python\\lib\\site-packages\\PIL\\Image.py:3083\u001b[0m, in \u001b[0;36mfromarray\u001b[1;34m(obj, mode)\u001b[0m\n\u001b[0;32m   3081\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m   3082\u001b[0m         msg \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mCannot handle this data type: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m typekey\n\u001b[1;32m-> 3083\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(msg) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[0;32m   3084\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   3085\u001b[0m     rawmode \u001b[39m=\u001b[39m mode\n",
      "\u001b[1;31mTypeError\u001b[0m: Cannot handle this data type: (1, 1, 3), <f8"
     ]
    }
   ],
   "source": [
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "def open_image(path):\n",
    "    image = Image.open(path)\n",
    "    image = transforms.ToTensor()(image)\n",
    "    return image\n",
    "def image_noiser(image):\n",
    "    image = image + torch.randn(image.shape).to(device)*0.1\n",
    "    return image\n",
    "def save_image(image,path):\n",
    "    image = image.cpu()\n",
    "    image = image.detach().numpy()\n",
    "    image = image.transpose(1,2,0)\n",
    "    image = (image*255).astype(np.uint8)\n",
    "    image = Image.fromarray(image)\n",
    "    image.save(path)\n",
    "    return image\n",
    "def get_all_images_from_folder(path):\n",
    "    images = []\n",
    "    for filename in os.listdir(path):\n",
    "        images.append(filename)\n",
    "    return images\n",
    "#Noisy\n",
    "# for images in get_all_images_from_folder(\"D:/Valentin/Desktop/Images ummt/raw/\"):\n",
    "#     save_image(image_noiser(open_image(\"D:/Valentin/Desktop/Images ummt/raw/\"+images)),\"D:/Valentin/Desktop/Images ummt/noisy/\"+images)\n",
    "#Wrong\n",
    "# folder = get_all_images_from_folder(\"D:/Valentin/Desktop/Images ummt/raw/\")\n",
    "# for i in range(len(folder)):\n",
    "#     save_image(open_image(\"D:/Valentin/Desktop/Images ummt/raw/\"+folder[i]),\"D:/Valentin/Desktop/Images ummt/wrong/\"+folder[(i+1)%len(folder)])\n",
    "#Blur\n",
    "!pip install opencv-python\n",
    "import cv2\n",
    "def blur_image(image):\n",
    "    image = image.cpu()\n",
    "    image = image.detach().numpy()\n",
    "    image = image.transpose(1,2,0)\n",
    "    image = (image*255).astype(np.uint8)\n",
    "    image = cv2.blur(image,(5,5))\n",
    "    image = Image.fromarray(image)\n",
    "    image = transforms.ToTensor()(image)\n",
    "    return image\n",
    "for images in get_all_images_from_folder(\"D:/Valentin/Desktop/Images ummt/raw/\"):\n",
    "    save_image(blur_image(open_image(\"D:/Valentin/Desktop/Images ummt/raw/\"+images)),\"D:/Valentin/Desktop/Images ummt/blurred/\"+images)\n",
    "#Negative\n",
    "def negative_image(image):\n",
    "    image = image.cpu()\n",
    "    image = image.detach().numpy()\n",
    "    image = image.transpose(1,2,0)\n",
    "    image = (image*255).astype(np.uint8)\n",
    "    image = 255-image\n",
    "    image = Image.fromarray(image)\n",
    "    image = transforms.ToTensor()(image)\n",
    "    return image\n",
    "for images in get_all_images_from_folder(\"D:/Valentin/Desktop/Images ummt/raw/\"):   \n",
    "    save_image(negative_image(open_image(\"D:/Valentin/Desktop/Images ummt/raw/\"+images)),\"D:/Valentin/Desktop/Images ummt/negative/\"+images)\n",
    "#Rotation\n",
    "def rotate_image(image):\n",
    "    image = image.cpu()\n",
    "    image = image.detach().numpy()\n",
    "    image = image.transpose(1,2,0)\n",
    "    image = (image*255).astype(np.uint8)\n",
    "    image = cv2.rotate(image,cv2.ROTATE_90_CLOCKWISE)\n",
    "    image = Image.fromarray(image)\n",
    "    image = transforms.ToTensor()(image)\n",
    "    return image\n",
    "for images in get_all_images_from_folder(\"D:/Valentin/Desktop/Images ummt/raw/\"):\n",
    "    save_image(rotate_image(open_image(\"D:/Valentin/Desktop/Images ummt/raw/\"+images)),\"D:/Valentin/Desktop/Images ummt/rotated/\"+images)\n",
    "#Contrast\n",
    "def contrast_image(image):\n",
    "    image = image.cpu()\n",
    "    image = image.detach().numpy()\n",
    "    image = image.transpose(1,2,0)\n",
    "    image = (image*255).astype(np.uint8)\n",
    "    # image = cv2.equalizeHist(image)\n",
    "    image= cv2.convertScaleAbs(image, 10, 2)\n",
    "\n",
    "    image = Image.fromarray(image)\n",
    "    image = transforms.ToTensor()(image)\n",
    "    return image\n",
    "for images in get_all_images_from_folder(\"D:/Valentin/Desktop/Images ummt/raw/\"):\n",
    "    save_image(contrast_image(open_image(\"D:/Valentin/Desktop/Images ummt/raw/\"+images)),\"D:/Valentin/Desktop/Images ummt/contrast/\"+images)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
