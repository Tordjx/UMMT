{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "efb10bae-68df-4d91-9e10-2cecaf169e71",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\valen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# !git clone https://github.com/tordjx/ummt.git\n",
    "# import os \n",
    "# os.chdir(\"ummt/Core model files\")\n",
    "# os.getcwd()\n",
    "!pip install matplotlib --quiet\n",
    "!pip install livelossplot --quiet\n",
    "!pip install nltk --quiet\n",
    "!pip install torchtext --quiet\n",
    "#mc cp s3/tordjx ummt/Core*model*files --recursive\n",
    "# watch -n 0.5 nvidia-smi\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "image_bool = False\n",
    "load_model = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f7ee3bb-7c9c-47e3-ae7f-064be78e7002",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#%%\n",
    "from Modele_decodeur_maison import *\n",
    "from Pipeline import *\n",
    "from Trainer import * \n",
    "from greedy_beam_search import *\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "batch_size = 256\n",
    "\n",
    "# Texts\n",
    "tokenized_fr,tokenized_en, vocab_fr,vocab_en = get_train_data_nouveau(batch_size,True)\n",
    "tokenized_val_fr,tokenized_val_en, _,_ = get_train_data_nouveau(batch_size,False)\n",
    "#Data non batchés\n",
    "n_token_fr = len(vocab_fr.keys())\n",
    "n_token_en = len(vocab_en.keys())\n",
    "\n",
    "inv_map_en = {v: k for k, v in vocab_en.items()}\n",
    "inv_map_fr = {v: k for k, v in vocab_fr.items()}\n",
    "\n",
    "n_head =8\n",
    "num_encoder_layers = 4\n",
    "num_decoder_layers = 4\n",
    "dim_feedforward = 512\n",
    "dropout = 0.1\n",
    "activation = nn.Softmax(dim=2)\n",
    "embedding_dim = 128\n",
    "\n",
    "model_fr = Modèle(n_token_fr,embedding_dim,n_head, num_encoder_layers,num_decoder_layers,dim_feedforward,dropout,activation,vocab_fr[\"TOKEN_VIDE\"],vocab_fr[\"DEBUT_DE_PHRASE\"],vocab_fr[\"FIN_DE_PHRASE\"],False,'').to(device)\n",
    "model_en = Modèle(n_token_en,embedding_dim,n_head, num_encoder_layers,num_decoder_layers,dim_feedforward,dropout,activation,vocab_en[\"TOKEN_VIDE\"],vocab_en[\"DEBUT_DE_PHRASE\"],vocab_en[\"FIN_DE_PHRASE\"],False,'').to(device)\n",
    "model_fr.feedforward= model_en.feedforward\n",
    "for i in range(3):\n",
    "  model_fr.decoder.layers[i]= model_en.decoder.layers[i]\n",
    "  model_fr.encoder.layers[i]= model_en.encoder.layers[i]\n",
    "prefix = \"fulltry\"\n",
    "if load_model : \n",
    "    model_en.load_state_dict(torch.load(\"tordjx/\"+prefix+\"_en\"))\n",
    "    model_fr.load_state_dict(torch.load(\"tordjx/\"+prefix+\"_fr\"))\n",
    "else : \n",
    "    with open(\"logs.txt\",'w') as logs :\n",
    "        logs.write(\"\")\n",
    "        logs.close()\n",
    "if image_bool :\n",
    "    train_features  = np.load(\"tordjx/train-resnet50-res4frelu.npy\")\n",
    "    val_features = np.load(\"tordjx/val-resnet50-res4frelu.npy\")\n",
    "    train_features = torch.from_numpy(train_features)\n",
    "    val_features = torch.from_numpy(val_features)\n",
    "    train_data_fr = [tokenized_fr, train_features]\n",
    "    train_data_en = [tokenized_en, train_features]\n",
    "    val_data_fr = [tokenized_val_fr, val_features]\n",
    "    val_data_en = [tokenized_val_en, val_features]\n",
    "def save_model(model_en,model_fr,prefix):\n",
    "    torch.save(model_fr.state_dict(), \"tordjx/\"+prefix+\"_fr\")\n",
    "    torch.save(model_en.state_dict(), \"tordjx/\"+prefix+\"_en\")\n",
    "    import os\n",
    "    import s3fs\n",
    "    !pip install pandas\n",
    "    import pandas\n",
    "    # Create filesystem object\n",
    "    S3_ENDPOINT_URL = \"https://\" + os.environ[\"AWS_S3_ENDPOINT\"]\n",
    "    fs = s3fs.S3FileSystem(client_kwargs={'endpoint_url': S3_ENDPOINT_URL})\n",
    "    fs.upload(\"tordjx/\"+prefix+\"_fr\",\"tordjx/\"+prefix+\"_fr\")\n",
    "    fs.upload(\"tordjx/\"+prefix+\"_en\",\"tordjx/\"+prefix+\"_en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d6a5f97-980c-4051-a19b-422479724a88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Programmes\\Python\\lib\\site-packages\\torch\\nn\\functional.py:4999: UserWarning: Support for mismatched src_key_padding_mask and src_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n",
      "d:\\Programmes\\Python\\lib\\site-packages\\torch\\nn\\functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'batch_size' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 18\u001b[0m\n\u001b[0;32m     16\u001b[0m     mixed_train(val_data_en,val_data_fr,inv_map_en,inv_map_fr,model_fr,model_en,train_data_fr,train_data_en,\u001b[39m60\u001b[39m,batch_size, \u001b[39mTrue\u001b[39;00m,\u001b[39m1\u001b[39m\u001b[39m/\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[0;32m     17\u001b[0m \u001b[39melse\u001b[39;00m :\n\u001b[1;32m---> 18\u001b[0m     mixed_train(tokenized_val_en,tokenized_val_fr,inv_map_en,inv_map_fr,model_fr,model_en,tokenized_fr,tokenized_en,\u001b[39m60\u001b[39;49m,batch_size, \u001b[39mFalse\u001b[39;49;00m,\u001b[39m1\u001b[39;49m\u001b[39m/\u001b[39;49m\u001b[39m2\u001b[39;49m)\n\u001b[0;32m     19\u001b[0m torch\u001b[39m.\u001b[39msave(model_fr\u001b[39m.\u001b[39mstate_dict(), \u001b[39m\"\u001b[39m\u001b[39mtordjx/\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m+\u001b[39mprefix\u001b[39m+\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m_fr\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     20\u001b[0m torch\u001b[39m.\u001b[39msave(model_en\u001b[39m.\u001b[39mstate_dict(), \u001b[39m\"\u001b[39m\u001b[39mtordjx/\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m+\u001b[39mprefix\u001b[39m+\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m_en\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32md:\\Valentin\\Documents\\ENSAE\\STATAPP\\UMMT\\Core model files\\Trainer.py:99\u001b[0m, in \u001b[0;36mmixed_train\u001b[1;34m(val_data_en, val_data_fr, inv_map_en, inv_map_fr, model_fr, model_en, train_data_fr, train_data_en, n_iter, batch_size, image_bool, part_auto_encoding)\u001b[0m\n\u001b[0;32m     97\u001b[0m tokenized_val_fr \u001b[39m=\u001b[39m val_data_fr[\u001b[39m0\u001b[39m]\n\u001b[0;32m     98\u001b[0m \u001b[39mfor\u001b[39;00m i_iter \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_iter):\n\u001b[1;32m---> 99\u001b[0m     save_dataframe_eval(model_fr,model_en,tokenized_val_en,tokenized_val_fr,inv_map_en,inv_map_fr,image_bool,i_iter)\n\u001b[0;32m    100\u001b[0m     model_en\u001b[39m.\u001b[39mcurr_epoch \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m\n\u001b[0;32m    101\u001b[0m     model_fr\u001b[39m.\u001b[39mcurr_epoch \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m\n",
      "File \u001b[1;32md:\\Valentin\\Documents\\ENSAE\\STATAPP\\UMMT\\Core model files\\evaluateur.py:197\u001b[0m, in \u001b[0;36msave_dataframe_eval\u001b[1;34m(model_fr, model_en, tokenized_val_en, tokenized_val_fr, inv_map_en, inv_map_fr, image_bool, epoch)\u001b[0m\n\u001b[0;32m    196\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msave_dataframe_eval\u001b[39m(model_fr,model_en,tokenized_val_en,tokenized_val_fr,inv_map_en,inv_map_fr,image_bool,epoch \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m):\n\u001b[1;32m--> 197\u001b[0m     df \u001b[39m=\u001b[39m dataframe_eval(model_fr,model_en,tokenized_val_en,tokenized_val_fr,inv_map_en,inv_map_fr,image_bool)\n\u001b[0;32m    198\u001b[0m     df[\u001b[39m\"\u001b[39m\u001b[39mbleu_en_fr\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m row: bleu(row,\u001b[39m\"\u001b[39m\u001b[39men\u001b[39m\u001b[39m\"\u001b[39m), axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m    199\u001b[0m     df[\u001b[39m\"\u001b[39m\u001b[39mbleu_fr_en\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m row: bleu(row,\u001b[39m\"\u001b[39m\u001b[39mfr\u001b[39m\u001b[39m\"\u001b[39m), axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32md:\\Valentin\\Documents\\ENSAE\\STATAPP\\UMMT\\Core model files\\evaluateur.py:127\u001b[0m, in \u001b[0;36mdataframe_eval\u001b[1;34m(model_fr, model_en, val_data_en, val_data_fr, inv_map_en, inv_map_fr, image_bool)\u001b[0m\n\u001b[0;32m    125\u001b[0m \u001b[39melse\u001b[39;00m :\n\u001b[0;32m    126\u001b[0m     val_data_en,val_data_fr \u001b[39m=\u001b[39m val_data_en,val_data_fr\n\u001b[1;32m--> 127\u001b[0m batched_data_en,batched_data_fr\u001b[39m=\u001b[39mbatchify([val_data_en,val_data_fr],batch_size,image_bool,conservative\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,permute \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m    128\u001b[0m \u001b[39mif\u001b[39;00m image_bool:\n\u001b[0;32m    129\u001b[0m     src,features \u001b[39m=\u001b[39m batched_data_en\n",
      "\u001b[1;31mNameError\u001b[0m: name 'batch_size' is not defined"
     ]
    }
   ],
   "source": [
    "#On va entrainer 4 modeles : avec/sans images, avec/sans teacher forcing\n",
    "teacher_forcing = False\n",
    "bools = [True,False]\n",
    "for image_bool in bools : \n",
    "    prefix = str(image_bool)+str(teacher_forcing)\n",
    "    model_fr = Modèle(n_token_fr,embedding_dim,n_head, num_encoder_layers,num_decoder_layers,dim_feedforward,dropout,activation,vocab_fr[\"TOKEN_VIDE\"],vocab_fr[\"DEBUT_DE_PHRASE\"],vocab_fr[\"FIN_DE_PHRASE\"],teacher_forcing,prefix).to(device)\n",
    "    model_en = Modèle(n_token_en,embedding_dim,n_head, num_encoder_layers,num_decoder_layers,dim_feedforward,dropout,activation,vocab_en[\"TOKEN_VIDE\"],vocab_en[\"DEBUT_DE_PHRASE\"],vocab_en[\"FIN_DE_PHRASE\"],teacher_forcing,prefix).to(device)\n",
    "    model_fr.feedforward= model_en.feedforward\n",
    "    for i in range(3):\n",
    "        model_fr.decoder.layers[i]= model_en.decoder.layers[i]\n",
    "        model_fr.encoder.layers[i]= model_en.encoder.layers[i]\n",
    "    with open(prefix+\"logs.txt\",'w') as logs :\n",
    "        logs.write(\"\")\n",
    "        logs.close()\n",
    "    if image_bool :\n",
    "        mixed_train(val_data_en,val_data_fr,inv_map_en,inv_map_fr,model_fr,model_en,train_data_fr,train_data_en,60,batch_size, True,1/2)\n",
    "    else :\n",
    "        mixed_train(tokenized_val_en,tokenized_val_fr,inv_map_en,inv_map_fr,model_fr,model_en,tokenized_fr,tokenized_en,60,batch_size, False,1/2)\n",
    "    torch.save(model_fr.state_dict(), \"tordjx/\"+prefix+\"_fr\")\n",
    "    torch.save(model_en.state_dict(), \"tordjx/\"+prefix+\"_en\")\n",
    "    pd.DataFrame(model_en.loss_list).to_csv(\"tordjx/\"+prefix+\"loss_en.csv\")\n",
    "    pd.DataFrame(model_fr.loss_list).to_csv(\"tordjx/\"+prefix+\"loss_fr.csv\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba8bf1c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Programmes\\Python\\lib\\site-packages\\torch\\nn\\functional.py:4999: UserWarning: Support for mismatched src_key_padding_mask and src_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n",
      "d:\\Programmes\\Python\\lib\\site-packages\\torch\\nn\\functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.7672,  0.6324, -2.2318,  ...,  3.2074, -1.5971,  0.5021],\n",
       "         [-1.8711, -1.1175, -1.7387,  ...,  1.9533, -0.6104,  0.6691],\n",
       "         [-2.5070,  0.7686, -1.9276,  ...,  2.5660, -0.9294,  0.5455],\n",
       "         ...,\n",
       "         [-2.2627,  0.6238, -1.7786,  ...,  2.0241, -1.1009,  1.6198],\n",
       "         [-1.3301,  0.1612, -1.9499,  ...,  1.6829, -1.0265,  1.4370],\n",
       "         [-1.4545,  0.3633, -1.3041,  ...,  1.8026, -1.3168,  1.2916]],\n",
       "\n",
       "        [[ 0.4346, -1.8247, -2.3207,  ...,  2.7586, -0.3141, -0.0385],\n",
       "         [-0.2487, -0.9901, -1.9830,  ...,  2.4211, -0.4552,  0.7975],\n",
       "         [-0.5314, -0.5862, -1.6345,  ...,  2.3901, -0.4316,  0.7297],\n",
       "         ...,\n",
       "         [-1.5806, -0.3029, -1.5977,  ...,  3.0642, -1.3033,  0.8397],\n",
       "         [-1.0783, -0.6023, -2.4334,  ...,  1.8654, -0.2117,  0.9085],\n",
       "         [-1.1355, -0.4256, -1.5829,  ...,  3.0951, -1.1788,  1.3038]],\n",
       "\n",
       "        [[-0.4734, -1.1233, -2.7201,  ...,  3.4664,  0.1761,  0.8877],\n",
       "         [-1.1830, -1.4720, -1.6237,  ...,  2.1842, -0.6953, -0.3005],\n",
       "         [-1.6022, -1.3293, -1.7169,  ...,  1.5716, -0.1938,  0.9530],\n",
       "         ...,\n",
       "         [-2.2047, -1.1574, -1.9742,  ...,  2.6724,  0.5779,  1.2782],\n",
       "         [-1.8340, -0.8617, -1.7533,  ...,  2.0062,  0.3763,  0.3884],\n",
       "         [-1.8117, -0.9105, -1.2268,  ...,  1.7089, -0.2627,  0.6535]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-1.7445, -1.0802, -3.8266,  ...,  2.8141, -0.1897,  0.1665],\n",
       "         [-1.4566, -1.8985, -3.2853,  ...,  2.9236, -0.1663, -0.1350],\n",
       "         [-0.9470, -1.6510, -3.5528,  ...,  2.8092, -0.2734,  0.0116],\n",
       "         ...,\n",
       "         [ 0.4313, -1.4283, -2.5295,  ...,  2.1533, -0.5493,  0.8083],\n",
       "         [ 0.5044, -0.7629, -2.9392,  ...,  1.6745,  0.2206,  0.4683],\n",
       "         [-0.3311, -1.0687, -3.0374,  ...,  1.7406,  0.0568,  0.4826]],\n",
       "\n",
       "        [[-0.7750, -1.4568, -3.1031,  ...,  2.6716, -0.4005,  0.4410],\n",
       "         [ 0.3063, -1.9225, -2.6522,  ...,  3.1562, -0.5556,  0.4125],\n",
       "         [-0.4148, -1.9725, -2.8228,  ...,  2.2459, -0.0563,  0.9737],\n",
       "         ...,\n",
       "         [-0.6264, -0.7993, -2.9419,  ...,  2.6457,  0.2646,  1.3802],\n",
       "         [-0.4947, -1.6511, -3.1860,  ...,  2.0097,  0.7460,  1.2256],\n",
       "         [-0.5192, -1.0773, -2.6038,  ...,  2.2966,  0.0607,  1.2168]],\n",
       "\n",
       "        [[ 0.0483, -1.3987, -3.2099,  ...,  1.8952, -0.8749, -0.2009],\n",
       "         [ 0.3864, -1.3319, -2.1818,  ...,  2.6513, -0.0176, -0.2622],\n",
       "         [-0.2619, -1.4960, -3.2397,  ...,  2.8554,  1.0315,  0.2707],\n",
       "         ...,\n",
       "         [-0.2402, -1.4280, -1.8668,  ...,  1.6698,  0.7460,  0.8663],\n",
       "         [-0.9940, -1.3050, -1.7044,  ...,  1.9627,  0.7954,  1.1541],\n",
       "         [-0.7783, -1.0880, -1.4609,  ...,  1.9946,  0.3037,  1.6723]]],\n",
       "       grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_en(tokenized_val_en[:batch_size],False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09bd9a6-d152-4dfa-984f-f9393a53199b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# if image_bool :\n",
    "#     loss_list = learning_rate_finder(val_data_en,val_data_fr,inv_map_en,inv_map_fr,model_fr,model_en,train_data_fr,train_data_en,150,batch_size, True,1/2)\n",
    "# else :\n",
    "#     loss_list = learning_rate_finder(tokenized_val_en,tokenized_val_fr,inv_map_en,inv_map_fr,model_fr,model_en,tokenized_fr,tokenized_en,150,batch_size, False,1/2)\n",
    "\n",
    "lrs = [10**(-10)*3**n for n in range(len(loss_list))]\n",
    "cut = 11\n",
    "begin = 10\n",
    "plt.plot(lrs[begin:-cut],loss_list[begin:-cut])\n",
    "plt.xscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033da8a6-1d98-4eea-be9a-ba67221f6780",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def moving_average(a, n=1,tail = 0) :\n",
    "    ret = np.cumsum(a, dtype=float)\n",
    "    ret[n:] = ret[n:] - ret[:-n]\n",
    "    \n",
    "    return (ret[n - 1:] / n)[tail:]\n",
    "\n",
    "plt.plot(moving_average(model_fr.loss_list))\n",
    "plt.plot(moving_average(model_en.loss_list))\n",
    "print(len(model_fr.loss_list)/(29000/batch_size))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a62a7311-d658-4917-8e00-35d585778982",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenized_val_en = val_data_en[0]\n",
    "i = np.random.randint(len(tokenized_val_en)//batch_size)\n",
    "j = np.random.randint(batch_size)\n",
    "src,features,tgt = donne_random(i,j,val_data_en,val_data_fr,batch_size,True)\n",
    "features = features.to(device,dtype=torch.float32)\n",
    "data = [src,features]\n",
    "# OUT= traduit('greedy',model_en,model_fr,data, inv_map_en,True,tgt[j],inv_map_fr,j)\n",
    "OUT = greedy_beam_search.CCF_greedy(model_en,model_fr,src, features, True)\n",
    "values,indices = torch.topk(OUT,2,2)\n",
    "print(values)\n",
    "print(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5ca3ea82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in d:\\programmes\\python\\lib\\site-packages (4.7.0.72)\n",
      "Requirement already satisfied: numpy>=1.21.2 in d:\\programmes\\python\\lib\\site-packages (from opencv-python) (1.24.2)\n"
     ]
    }
   ],
   "source": [
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "def open_image(path):\n",
    "    image = Image.open(path)\n",
    "    image = transforms.ToTensor()(image)\n",
    "    return image\n",
    "def image_noiser(image):\n",
    "    image = image + torch.randn(image.shape).to(device)*0.1\n",
    "    return image\n",
    "def save_image(image,path):\n",
    "    image = image.cpu()\n",
    "    image = image.detach().numpy()\n",
    "    image = image.transpose(1,2,0)\n",
    "    image = (image*255).astype(np.uint8)\n",
    "    image = Image.fromarray(image)\n",
    "    image.save(path)\n",
    "    return image\n",
    "def get_all_images_from_folder(path):\n",
    "    images = []\n",
    "    for filename in os.listdir(path):\n",
    "        images.append(filename)\n",
    "    return images\n",
    "#Noisy\n",
    "for images in get_all_images_from_folder(\"Images ummt/raw/\"):\n",
    "    save_image(image_noiser(open_image(\"Images ummt/raw/\"+images)),\"Images ummt/noisy/\"+images)\n",
    "#Wrong\n",
    "folder = get_all_images_from_folder(\"Images ummt/raw/\")\n",
    "for i in range(len(folder)):\n",
    "    save_image(open_image(\"Images ummt/raw/\"+folder[i]),\"Images ummt/wrong/\"+folder[(i+1)%len(folder)])\n",
    "#Blur\n",
    "!pip install opencv-python\n",
    "import cv2\n",
    "def blur_image(image):\n",
    "    image = image.cpu()\n",
    "    image = image.detach().numpy()\n",
    "    image = image.transpose(1,2,0)\n",
    "    image = (image*255).astype(np.uint8)\n",
    "    image = cv2.blur(image,(5,5))\n",
    "    image = Image.fromarray(image)\n",
    "    image = transforms.ToTensor()(image)\n",
    "    return image\n",
    "for images in get_all_images_from_folder(\"Images ummt/raw/\"):\n",
    "    save_image(blur_image(open_image(\"Images ummt/raw/\"+images)),\"Images ummt/blurred/\"+images)\n",
    "#Negative\n",
    "def negative_image(image):\n",
    "    image = image.cpu()\n",
    "    image = image.detach().numpy()\n",
    "    image = image.transpose(1,2,0)\n",
    "    image = (image*255).astype(np.uint8)\n",
    "    image = 255-image\n",
    "    image = Image.fromarray(image)\n",
    "    image = transforms.ToTensor()(image)\n",
    "    return image\n",
    "for images in get_all_images_from_folder(\"Images ummt/raw/\"):   \n",
    "    save_image(negative_image(open_image(\"Images ummt/raw/\"+images)),\"Images ummt/negative/\"+images)\n",
    "#Rotation\n",
    "def rotate_image(image):\n",
    "    image = image.cpu()\n",
    "    image = image.detach().numpy()\n",
    "    image = image.transpose(1,2,0)\n",
    "    image = (image*255).astype(np.uint8)\n",
    "    image = cv2.rotate(image,cv2.ROTATE_90_CLOCKWISE)\n",
    "    image = Image.fromarray(image)\n",
    "    image = transforms.ToTensor()(image)\n",
    "    return image\n",
    "for images in get_all_images_from_folder(\"Images ummt/raw/\"):\n",
    "    save_image(rotate_image(open_image(\"Images ummt/raw/\"+images)),\"Images ummt/rotated/\"+images)\n",
    "#Contrast\n",
    "def contrast_image(image):\n",
    "    image = image.cpu()\n",
    "    image = image.detach().numpy()\n",
    "    image = image.transpose(1,2,0)\n",
    "    image = (image*255).astype(np.uint8)\n",
    "    # image = cv2.equalizeHist(image)\n",
    "    image= cv2.convertScaleAbs(image, 10, 2)\n",
    "\n",
    "    image = Image.fromarray(image)\n",
    "    image = transforms.ToTensor()(image)\n",
    "    return image\n",
    "for images in get_all_images_from_folder(\"Images ummt/raw/\"):\n",
    "    save_image(contrast_image(open_image(\"Images ummt/raw/\"+images)),\"Images ummt/contrast/\"+images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "dde2f4f8",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'gate.'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[96], line 29\u001b[0m\n\u001b[0;32m     27\u001b[0m longueur_max \u001b[39m=\u001b[39m \u001b[39m64\u001b[39m\n\u001b[0;32m     28\u001b[0m text \u001b[39m=\u001b[39m [[phrase[i] \u001b[39mif\u001b[39;00m i \u001b[39m<\u001b[39m \u001b[39mlen\u001b[39m(phrase) \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mTOKEN_VIDE\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m (longueur_max)] \u001b[39mfor\u001b[39;00m phrase \u001b[39min\u001b[39;00m text]\n\u001b[1;32m---> 29\u001b[0m text \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor([[vocab_en[mot] \u001b[39mfor\u001b[39;00m mot \u001b[39min\u001b[39;00m phrase] \u001b[39mfor\u001b[39;00m phrase \u001b[39min\u001b[39;00m text])\u001b[39m.\u001b[39mto(device \u001b[39m=\u001b[39m device, dtype \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mlong)\n",
      "Cell \u001b[1;32mIn[96], line 29\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     27\u001b[0m longueur_max \u001b[39m=\u001b[39m \u001b[39m64\u001b[39m\n\u001b[0;32m     28\u001b[0m text \u001b[39m=\u001b[39m [[phrase[i] \u001b[39mif\u001b[39;00m i \u001b[39m<\u001b[39m \u001b[39mlen\u001b[39m(phrase) \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mTOKEN_VIDE\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m (longueur_max)] \u001b[39mfor\u001b[39;00m phrase \u001b[39min\u001b[39;00m text]\n\u001b[1;32m---> 29\u001b[0m text \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor([[vocab_en[mot] \u001b[39mfor\u001b[39;00m mot \u001b[39min\u001b[39;00m phrase] \u001b[39mfor\u001b[39;00m phrase \u001b[39min\u001b[39;00m text])\u001b[39m.\u001b[39mto(device \u001b[39m=\u001b[39m device, dtype \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mlong)\n",
      "Cell \u001b[1;32mIn[96], line 29\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     27\u001b[0m longueur_max \u001b[39m=\u001b[39m \u001b[39m64\u001b[39m\n\u001b[0;32m     28\u001b[0m text \u001b[39m=\u001b[39m [[phrase[i] \u001b[39mif\u001b[39;00m i \u001b[39m<\u001b[39m \u001b[39mlen\u001b[39m(phrase) \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mTOKEN_VIDE\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m (longueur_max)] \u001b[39mfor\u001b[39;00m phrase \u001b[39min\u001b[39;00m text]\n\u001b[1;32m---> 29\u001b[0m text \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor([[vocab_en[mot] \u001b[39mfor\u001b[39;00m mot \u001b[39min\u001b[39;00m phrase] \u001b[39mfor\u001b[39;00m phrase \u001b[39min\u001b[39;00m text])\u001b[39m.\u001b[39mto(device \u001b[39m=\u001b[39m device, dtype \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mlong)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'gate.'"
     ]
    }
   ],
   "source": [
    "with open('Images ummt/captions.txt','r') as file : \n",
    "    lines = file.readlines()\n",
    "    file.close()\n",
    "def get_resnet_features_from_image(image):\n",
    "    resnet = models.resnet152(pretrained=True)\n",
    "    resnet.eval()\n",
    "    resnet = resnet.to(device)\n",
    "    resnet = nn.Sequential(*list(resnet.children())[:-1])\n",
    "    image = image.to(device)\n",
    "    image = image.unsqueeze(0)\n",
    "    image = resnet(image)\n",
    "    image = image.squeeze(0)\n",
    "    return image\n",
    "\n",
    "lines = [line[:-1].split(',') for line in  lines]\n",
    "images= [line[0] for line in lines]\n",
    "captions = [line[1][1:] for line in lines]\n",
    "folder =[x for x in os.listdir('Images ummt') if x != 'captions.txt']\n",
    "text=[]\n",
    "features=[]\n",
    "for name in folder :\n",
    "    text+=captions\n",
    "    # for image in images: \n",
    "    #     features.append(get_resnet_features_from_image(open_image('Images ummt/'+name+'/'+image)))\n",
    "text = [[\"DEBUT_DE_PHRASE\"]+ligne.strip().split(\" \")+[\"FIN_DE_PHRASE\"] for ligne in text ]\n",
    "longueur_max = 64\n",
    "text = [[phrase[i] if i < len(phrase) else \"TOKEN_VIDE\" for i in range (longueur_max)] for phrase in text]\n",
    "text = torch.tensor([[vocab_en[mot] for mot in phrase] for phrase in text]).to(device = device, dtype = torch.long)\n",
    "text = torch.cat((text, torch.zeros(batch_size-text.shape[0]).to(device = device, dtype = torch.long)))\n",
    "features = torch.cat(features,torch.zeros(batch_size-features.shape[0],features.shape[1],features.shape[2],features.shape[2]).to(device = device, dtype = torch.float))\n",
    "#Pour la suite : faire un machin qui fait un dataframe avec la traduction, la source, l'image, le nom de la transformation, le bleu score, l'image source, et l'attention image et text\n",
    "#Essayer de modifier l'output du decoder pour faire en sorte qu'il output aussi l'attention\n",
    "#se baser sur le programme traduction dans evaluateur et ça devrait aller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "59167376",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "filter = torch.rand(14,14)\n",
    "image = open_image('Images ummt/raw/10350842.jpg')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_rgb_image(image):\n",
    "    image = image.cpu()\n",
    "    image = image.detach().numpy()\n",
    "    image = image.transpose(1,2,0)\n",
    "    plt.imshow(image)\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "def plot_attention_on_image(image,attention):\n",
    "    image = image.cpu()\n",
    "    image = image.detach().numpy()\n",
    "    image = image.transpose(1,2,0)\n",
    "    plt.imshow(image)\n",
    "    plt.imshow(attention,alpha = 0.5,cmap = 'Reds')\n",
    "    plt.show()\n",
    "plot_attention_on_image(image,filter)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
