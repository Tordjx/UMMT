{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries\n",
    "import torch\n",
    "import clip\n",
    "from PIL import Image\n",
    "import os\n",
    "from torchvision.datasets import CIFAR100\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import random as rd\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get correspondance between images and captions\n",
    "\n",
    "def get_captions():\n",
    "    # file names\n",
    "    file = open(\"C:/Users/lucas/Documents/GitHub/dataset/data/task1/image_splits/train.txt\")\n",
    "    names = [line[:-1] for line in file ]\n",
    "    file.close()\n",
    "    # Captions\n",
    "    file = open(\"C:/Users/lucas/Desktop/train.en\")\n",
    "    captions = [line[:-1] for line in file ]\n",
    "    file.close()\n",
    "    return names, captions\n",
    "\n",
    "names, captions = get_captions()\n",
    "dict_captions = { names[i] : captions[i] for i in range(len(names)) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, preprocess = clip.load('ViT-B/32', device)\n",
    "\n",
    "def similarity_prediction(file_name, nb=1):\n",
    "    # image \n",
    "    im = Image.open(r\"C:/Users/lucas/Downloads/flickr30k-images.tar/flickr30k-images/\"+ file_name) \n",
    "    image = preprocess(im).unsqueeze(0).to(device)\n",
    "    # text \n",
    "    texts = clip.tokenize(names[:100]).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        image_features = model.encode_image(image)\n",
    "        text_features = model.encode_text(texts)\n",
    "\n",
    "    image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "    similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "\n",
    "    values, indices = similarity[0].topk(nb)\n",
    "    print(\"Top predictions:\")\n",
    "    for value, index in zip(values, indices):\n",
    "        print(\"Caption nÂ°\" + str(index.item()) + \" : \" + str(value.item()))\n",
    "\n",
    "    return values, indices\n",
    "\n",
    "# Tests\n",
    "# c = 0\n",
    "# for i in range(100):\n",
    "#     values, indices = similarity_prediction(names[i])\n",
    "#     for value, index in zip(values, indices):\n",
    "#         if index.item() == i:\n",
    "#             c += 1\n",
    "# print(c/100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similarity score for each image-text couple\n",
    "\n",
    "model, preprocess = clip.load('ViT-B/32', device)\n",
    "\n",
    "def similarity_score():\n",
    "    simi_scores = {}\n",
    "\n",
    "    for file_name, caption in dict_captions.items():\n",
    "        # image\n",
    "        im = Image.open(r\"C:/Users/lucas/Downloads/flickr30k-images.tar/flickr30k-images/\"+ file_name) \n",
    "        image = preprocess(im).unsqueeze(0).to(device)\n",
    "        # text \n",
    "        text = clip.tokenize([caption]).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            image_features = model.encode_image(image)\n",
    "            text_features = model.encode_text(text)\n",
    "\n",
    "        image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "        text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "        similarity = (100.0 * image_features @ text_features.T)\n",
    "\n",
    "        simi_scores[file_name] = similarity.item()\n",
    "    \n",
    "    return simi_scores\n",
    "\n",
    "simi_scores = similarity_score()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparaison with other values\n",
    "\n",
    "def similarity_comparaison(file_name,nb=10,printing=False):\n",
    "\n",
    "    im = Image.open(r\"C:/Users/lucas/Downloads/flickr30k-images.tar/flickr30k-images/\"+ file_name) \n",
    "    image = preprocess(im).unsqueeze(0).to(device) \n",
    "    caption = dict_captions[file_name]\n",
    "    text = clip.tokenize([caption]).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        image_features = model.encode_image(image)\n",
    "        text_features = model.encode_text(text)\n",
    "        image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "        text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "        similarity = (100.0 * image_features @ text_features.T)\n",
    "\n",
    "    other_captions = np.random.choice(list(captions),size=nb)\n",
    "    similarities = []\n",
    "    for cap in other_captions:\n",
    "        cap = clip.tokenize([cap]).to(device)\n",
    "        with torch.no_grad():\n",
    "            cap_features = model.encode_text(cap)\n",
    "            cap_features /= cap_features.norm(dim=-1, keepdim=True)\n",
    "            similarities.append((100.0 * image_features @ cap_features.T).item())\n",
    "\n",
    "    if printing:\n",
    "        print(\"Real similarity : \" + str(similarity))\n",
    "        for i in range(len(similarities)):\n",
    "            print(\"Random caption \" + str(i) + \" : \" + str(similarities[i]))\n",
    "    \n",
    "    similarities.insert(0,similarity.item())\n",
    "    return similarities\n",
    "\n",
    "# Tests : \n",
    "results_comparaison = {}\n",
    "for file_name, caption in dict_captions.items():\n",
    "    results_comparaison[(file_name,caption)] = similarity_comparaison(file_name, printing=False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
